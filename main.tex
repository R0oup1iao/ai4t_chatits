\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{setspace}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\graphicspath{ {./figures/} }
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lineno}
\usepackage{booktabs}
\usepackage{rotating}
% \linenumbers


%%%%%% Bibliography %%%%%%
% Replace "sample" in the \addbibresource line below with the name of your .bib file.
\usepackage[style=nejm, 
citestyle=numeric-comp,
sorting=none]{biblatex}
\addbibresource{ref.bib}

%%%%%% Title %%%%%%
% Full titles can be a maximum of 200 characters, including spaces. 
% Title Format: Use title case, capitalizing the first letter of each word, except for certain small words, such as articles and short prepositions
\title{Harnessing Large Language Models for Adaptive and Explainable Traffic Forecasting}

%%%%%% Authors %%%%%%
% Authors should be listed in order of contribution to the paper, by full first name, then middle initial (if any), followed by last name and separated by commas.
% Please do not use initials for first names. If you use your middle name as a full name, use an initial for the first name and spell out your full middle name.
% Use a superscript asterisk (*) to identify the corresponding author and be sure to include that person’s e-mail address. Use symbols (in this order: †, ‡, §, ||, ¶, #, ††, ‡‡, etc.) for author notes, such as present addresses, “These authors contributed equally to this work” notations, and similar information.
% You can include group authors, but please include a list of the actual authors (the group members) in the Supplementary Materials.
\author[1]{Haoyang Yan}
\author[1,2]{Xiaolei Ma*}

%%%%%% Affiliations %%%%%%
\affil[1]{School of Transportation Science and Engineering, Beihang University, Beijing, China.}
\affil[2]{Key Laboratory of Intelligent Transportation Technology and System of the Ministry of Education, Beihang University, Beijing 102206, China}
\affil[*]{Address correspondence to: xiaolei@buaa.edu.cn}
% \affil[$\dag$]{These authors contributed equally to this work.}

%%%%%% Date %%%%%%
% Date is optional
\date{}

%%%%%% Spacing %%%%%%
% Use paragraph spacing of 1.5 or 2 (for double spacing, use command \doublespacing)
\onehalfspacing

\begin{document}

\maketitle

%%%%%% Abstract %%%%%%
\begin{abstract}
Accurate traffic state prediction is a cornerstone of Intelligent Transportation Systems (ITS). While deep learning models—specifically Graph Neural Networks (GNNs) and Transformers—have achieved state-of-the-art performance in routine forecasting, they exhibit significant fragility under anomalous conditions (e.g., accidents, extreme weather, public events) due to their reliance on historical stationarity. Furthermore, the "black-box" nature of these models precludes interpretability, limiting their operational utility. To address these deficiencies, this study proposes Chat-ITS, a novel hybrid framework that synergizes robust probabilistic time-series forecasting with the semantic reasoning capabilities of Large Language Models (LLMs). The methodology comprises three mathematically formalized stages: (1) A foundation model based on discrete tokenization and sequence-to-sequence learning generates a probabilistic distribution of future trajectories; (2) A cross-modal reasoning module, driven by an LLM, processes heterogeneous unstructured text data to perform Bayesian-like contextual adjustments on the candidate trajectories; (3) An operational interface generates explainable diagnostics and actionable control strategies. Extensive experiments on large-scale real-world datasets from Beijing demonstrate that Chat-ITS reduces prediction error by up to 15\% during non-recurring congestion events compared to baselines, while offering zero-shot generalization to unseen event types.
\end{abstract}

%%%%%% Main Text %%%%%%

\section{Introduction}
Accurate traffic prediction is fundamental to the efficacy of Intelligent Transportation Systems (ITS), enabling critical functions such as dynamic route guidance, adaptive traffic signal control, and proactive incident management essential for mitigating congestion, reducing emissions, and enhancing urban mobility resilience \cite{wu_big-data_2025}. Congestion alone costs economies billions annually and degrades quality of life in urban centers \cite{avila_data-driven_2020}. Effective ITS, powered by reliable forecasts, promises substantial improvements in transportation efficiency and sustainability. Recent advances, particularly the application of deep learning techniques like graph neural networks (GNNs) for modeling complex spatial dependencies across road networks \cite{shao_pre-training_2022} and sophisticated sequence models (e.g., temporal convolution networks, attention mechanisms) for capturing temporal dynamics \cite{li_transferable_2024, runge_inferring_2019}, have considerable improved short-term forecasting accuracy under typical, recurring traffic conditions \cite{yuan_unist_2024}. These methods effectively learn patterns from large historical datasets, providing a strong foundation for next-generation ITS applications operating under predictable circumstances.

Despite these successes, existing state-of-the-art traffic forecasting methods face critical limitations that hinder their real-world operational utility, particularly under non-routine circumstances \cite{guo_towards_2024, williams_context_2024, liu_time-mmd_2024}. Firstly, their predictive performance often degrades sharply during anomalous events such as road accidents, unexpected road closures, severe weather conditions, or large-scale public gatherings \cite{li_language_2025, xue_promptcast_2023}. Models trained primarily on routine historical patterns often exhibit poor generalization capabilities when faced with data distributions shifted by these irregular occurrences \cite{arango_chronosx_2025, zeng_are_2022}. This fragility undermines their reliability precisely when accurate prediction is most needed for effective incident response and management. Secondly, the fixed input encoding mechanisms of many deep learning models limit their ability to flexibly incorporate diverse, unstructured, or dynamic updates on road work schedules often contains crucial context for anticipating traffic impacts. Integrating textual incident reports, event schedules, social media alerts, or unforeseen disruptions often requires complex feature engineering or extensive model retraining, impeding adaptation to unforeseen event types without clear overhead \cite{tan_are_2024}. Thirdly, and perhaps most crucially for translation into practice, the standard output of these models, typically a high-dimensional matrix or tensor representing predicted speeds or flows, lacks direct interpretability. It fails to convey the underlying reasons for the predicted state or provide actionable guidance for traffic operators and decision-makers \cite{yuan_diffusion-ts_2023}. Consequently, even statistically accurate forecasts may not readily translate into effective, timely, and context-aware traffic management interventions, limiting the practical impact of these advanced techniques.

Large language models (LLMs) have emerged as powerful tools demonstrating remarkable capabilities in natural language understanding, contextual reasoning, and generalization across diverse tasks \cite{su_large_2024}. Their potential to process unstructured text, synthesize information from multiple sources, and generate human-like explanations offers promising avenues to address the challenges of context integration and interpretability in ITS \cite{wang_where_2023, guo_towards_2024, feng_citygpt_2024}. However, applying LLMs directly to the task of numerical time-series forecasting presents inherent difficulties. Their architectures, primarily optimized for sequential token generation, often struggle with the precise numerical regression required for traffic state prediction and can be inefficient in capturing the complex spatio-temporal statistical dependencies inherent in traffic flow \cite{tan_are_2024, liu_taming_2024}. Furthermore, training or even fine-tuning large LLMs for specialized forecasting tasks demands substantial computational resources and large-scale, domain-specific datasets, often proving impractical for widespread deployment in operational ITS settings where data characteristics can vary across locations and time \cite{jin_time-llm_2023}.

Here, we introduce Chat-ITS, a novel hybrid forecasting framework designed to bridge the gap between robust probabilistic time-series modeling and the contextual reasoning capabilities of LLMs, thereby overcoming the aforementioned limitations. Chat-ITS employs a synergistic, multi-stage approach that deliberately leverages the distinct strengths of each component. It first utilizes a dedicated spatio-temporal foundation model, pre-trained on extensive historical traffic data, to generate multiple candidate traffic state trajectories along with associated uncertainty estimates. This ensures statistical rigor and captures complex baseline traffic dynamics. Subsequently, an LLM, operating on these candidate trajectories, is conditioned on flexible natural language prompts. These prompts can seamlessly encode both structured data (e.g., quantitative weather forecasts, road closure notices with coordinates and times) and unstructured descriptions rich with linguistic cues (e.g., "Event update: sold-out show at the downtown arena, scheduled to end at 10 PM" or "Dispatch log: report of a multi-vehicle collision with emergency services responding on the northbound lane near exit 15"). The LLM evaluates the candidate trajectories within this broader context, reasoning about the likely impacts to select or adjust towards the most plausible outcome given the real-time information. Crucially, the LLM also generates human-readable explanations for its choice and actionable recommendations tailored for traffic management personnel, integrating insights potentially learned from historical operational data. This architecture deliberately avoids tasking the LLM with direct numerical prediction, instead harnessing its strengths in semantic comprehension, causal inference, and context-aware reasoning.

We demonstrate through comprehensive experiments encompassing both routine traffic patterns and a diverse set of simulated and real-world anomalous scenarios (including construction, accidents, and public events) that Chat-ITS noticeable outperforms conventional deep learning baseline models during irregular events, reducing prediction errors by up to 15\% under certain conditions, while matching state-of-the-art accuracy under normal conditions. Crucially, case studies highlight the framework's ability to generalize zero-shot to unseen event types described only via text prompts and deliver context-aware, actionable insights (e.g., suggesting specific signal timing adjustments, disseminating targeted traveler advisories, or recommending dynamic routing strategies). By integrating the statistical power of probabilistic forecasting with the semantic understanding and reasoning capabilities of language-based AI, Chat-ITS presents a new paradigm for traffic prediction, one that is not only accurate and adaptive but also explainable and directly aligned with the practical needs of transportation practitioners for effective real-world ITS deployment.

\section{Literature Review}

\subsection{Deep Learning for Traffic Forecasting}
Traffic forecasting has evolved from traditional statistical and regression-based approaches to deep learning models capable of capturing complex temporal dynamics. Early studies primarily relied on Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) architectures, to model sequential dependencies in traffic speed and flow data \cite{ma_long_2015, ma_forecasting_2021}. While effective in temporal modeling, these approaches are limited in their ability to explicitly represent the non-Euclidean spatial topology of road networks.

To address this limitation, Spatio-Temporal Graph Neural Networks (STGNNs) have been widely adopted. Representative models such as DCRNN \cite{li_diffusion_2017} and Graph WaveNet \cite{wu_graph_2019} combine graph convolutions with temporal sequence modeling to capture spatial correlations and temporal dynamics simultaneously. Subsequent works have extended these frameworks by relaxing the assumption of static spatial dependencies. For instance, Traffic Transformer introduces global–local decoders to hierarchically aggregate spatial features \cite{yan_learning_2022}, while PDFormer incorporates propagation delay-aware attention to explicitly model temporal lags in traffic interactions \cite{jiang_pdformer_2023}.

Despite their expressive power, the necessity of graph convolution has recently been questioned. Empirical studies indicate that simplified spatial modeling strategies can achieve comparable performance with substantially reduced computational overhead. SimST demonstrates that lightweight spatial aggregation approximates the effectiveness of GCN-based methods \cite{liu_we_2023}. Similarly, MLP-based architectures such as ST-MLP \cite{wang_st-mlp_2023} and STID \cite{shao_spatial-temporal_2022} show that concise spatio-temporal identity mappings can outperform complex GNNs by reducing overfitting to noisy spatial correlations.

In parallel, Transformer-based models have gained attention for their ability to capture long-range temporal dependencies. PatchTST segments time series into patches to preserve local temporal semantics \cite{nie_time_2023}, whereas iTransformer inverts the attention mechanism to better model multivariate correlations \cite{liu_itransformer_2023}. To further account for uncertainty and stochasticity in traffic systems, probabilistic generative approaches such as SpecSTG \cite{lin_specstg_2024} and Diffusion-TS \cite{yuan_diffusion-ts_2023} have been proposed, enabling uncertainty-aware forecasting and data imputation.

\subsection{Foundation Models for Time Series}
Inspired by the ``pre-train and fine-tune'' paradigm in Natural Language Processing, recent research has shifted toward the development of foundation models for time series analysis \cite{liang_foundation_2024, jin_large_2023}. These models aim to learn universal temporal representations that generalize across datasets and tasks, enabling zero-shot or few-shot inference.

Chronos adapts T5-style architectures by discretizing continuous values into token sequences, achieving strong zero-shot performance across diverse domains \cite{ansari_chronos_2024}. Lag-Llama adopts a probabilistic modeling framework to capture scaling behaviors over large-scale time-series corpora \cite{rasul_lag-llama_2023}. To model multi-periodicity, TimesNet reformulates one-dimensional time series into two-dimensional representations, facilitating variation modeling via convolutional kernels \cite{wu_timesnet_2023}.

Recent efforts further emphasize unified modeling across heterogeneous tasks. UniTS proposes a prompt-based backbone capable of jointly addressing forecasting, classification, and imputation \cite{gao_units_2024}, while Moirai-MoE introduces a Mixture-of-Experts architecture to handle diverse temporal resolutions without manual frequency alignment \cite{liu_moirai-moe_2024}. Another research direction explores the reuse of frozen pre-trained language or vision models. One Fits All demonstrates that large language models can be adapted to time series with minimal parameter updates \cite{zhou_one_2023}. Nevertheless, most existing foundation models operate exclusively on numerical signals, limiting their ability to incorporate unstructured contextual information, such as event descriptions, that is often critical for interpreting anomalies in intelligent transportation systems \cite{jin_time_2024}.

\subsection{Large Language Models in Transportation}
The application of Large Language Models (LLMs) in transportation research introduces a paradigm shift from purely numerical modeling toward semantic reasoning and agent-based decision making. A central challenge lies in aligning continuous time series data with the discrete token-based representations of LLMs. Time-LLM and LLM4TS address this issue through reprogramming and fine-tuning strategies that encode numerical sequences as language tokens \cite{jin_time-llm_2023, chang_llm4ts_2023}.

In urban computing scenarios, UrbanGPT integrates spatio-temporal dependency encoders with instruction tuning to improve generalization under data scarcity \cite{li_urbangpt_2024}, while ST-LLM reformulates spatio-temporal observations as token sequences to capture global network dependencies \cite{liu_spatial-temporal_2024}. Beyond forecasting, LLMs have been explored for explanation, simulation, and decision support. TF-LLM and ChatTraffic generate natural language interpretations of traffic conditions and congestion causes, enhancing model interpretability \cite{guo_explainable_2024, zhang_chattraffic_2024}. CityGPT further extends this idea by constructing a city-scale world model in which LLM-based agents perform diverse urban tasks \cite{feng_citygpt_2024}.

More advanced frameworks adopt ``LLM-in-the-loop'' architectures. TimeCAP and TimeXL employ multi-agent systems in which LLMs generate contextual summaries or reasoning paths that guide downstream numerical predictors \cite{lee_timecap_2025, jiang_explainable_2025}. Additionally, external information sources such as social events and news reports have been incorporated via generative agents to stabilize forecasting under non-stationary conditions \cite{wang_news_2024}. Nevertheless, a fundamental challenge remains in effectively reconciling the numerical accuracy of specialized time-series models with the high-level semantic reasoning capabilities of LLMs, particularly for real-time anomaly detection and intervention.


\section{Preliminary}
Traffic prediction is typically framed as a short-term time-series forecasting task, where future values $\mathbf{X}_{T+1:T+n}$ are predicted based on historical observations $\mathbf{X}_{1:T}$. This paper tackles a multi-modal version of this problem, recognizing that real-world traffic dynamics are influenced not only by past traffic states but also by a plethora of contextual factors often conveyed through textual or structured non-time-series data. We work with input instances $(\mathbf{X}_{1:T}, \mathbf{s})$, consisting of historical time series data $\mathbf{X}_{1:T} = \{\mathbf{x}_1, \ldots, \mathbf{x}_T\}$, where each $\mathbf{x}_t \in \mathbb{R}^N$ captures $D$ features of traffic states (e.g., speed, flow, occupancy) for $N$ spatial locations (e.g., road segments, sensors) over $T$ historical time steps, and auxiliary contextual information $\mathbf{s}$. This contextual information $\mathbf{s}$ can be diverse, including structured data (e.g., weather parameters, event schedules, road work logs) and unstructured natural language text (e.g., incident reports, social media alerts, news feeds) that potentially influences the time series and provides valuable context for improving forecast accuracy, especially during non-routine conditions. Our objective is to develop a model $\mathcal{F}$ that these multi-modal inputs to accurate and reliable predictions of future traffic states, potentially including uncertainty quantification. This is formalized as:
\begin{equation} \label{eq:1}
    \mathbf{X}_{T+1:T+n} = \{\mathbf{x}_{T+1}, \mathbf{x}_{T+2}, \ldots, \mathbf{x}_{T+n}\} = \mathcal{F}(\mathbf{X}_{1:T}, \mathbf{s}),
\end{equation}
where $\mathbf{X}_{T+1:T+n}$ is the predicted sequence of $n$ future state vectors or distributions. The ultimate goal is to identify an optimal model $\mathcal{F}$ that delivers accurate and reliable predictions while also being explainable and effectively leveraging the contextual information from $\mathbf{s}$ to adapt to both routine and non-routine conditions.

\section{Methodology}
\subsection{Overall Framework}
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{overview.pdf}
    \caption{Overall architecture of the Chat-ITS framework. (A) Stage 1: A pre-trained spatio-temporal foundation model processes historical traffic data $\mathbf{X}_{1:T}$ to generate multiple candidate future trajectories $\{\hat{\mathbf{X}}^{(k)}\}$ representing a baseline probabilistic forecast. (B) Stage 2: Real-time contextual information $\mathbf{s}$, including structured and unstructured event data, is processed by an LLM. The LLM reasons about the event's impact and evaluates the candidate trajectories, selecting or adjusting to the most plausible event-conditioned forecast $\hat{\mathbf{X}}^*$. (C) Stage 3: The adjusted forecast, along with historical dispatch patterns, feeds into the LLM to generate human-readable summary reports and actionable traffic management recommendations.}
    \label{fig:1}
\end{figure}

The Chat-ITS framework, depicted schematically in Fig.\ref{fig:1},operates through three synergistic core stages designed to integrate the strengths of advanced time-series modeling and large language models: (1) Foundational Probabilistic Forecasting, (2) LLM-Enhanced Contextual Adjustment, and (3) LLM-Powered Reporting and Decision Support.

\begin{itemize}
    \item \textbf{Stage 1: Foundational Probabilistic Forecasting (Fig.\ref{fig:1} A)}: The foundation of Chat-ITS is a robust forecasting model capable of capturing complex dependencies in traffic data and providing probabilistic outputs. We employ a state-of-the-art architecture pre-trained on extensive historical traffic data. To ensure the model learns representative patterns, the pre-training data include curated subsets, such as: (i) time-series from high-volume road segments or grid cells representing typical urban traffic dynamics, and (ii) traffic data aggregated around key venues (stadiums, transport hubs, event centers) known to generate non-standard patterns. Inspired by architectures like Chronos \cite{ansari_chronos_2024} which adapt language transformer-based models for time-series, our foundation model processes the historical input and generates not a single prediction, but multiple trajectory samples $\{\hat{\mathbf{X}}^{(k)}_{T+1:T+n}\}_{k=1}^K$. These samples collectively approximate the predictive distribution $P(\mathbf{X}_{T+1:T+n} | \mathbf{X}_{1:T})$, providing a baseline probabilistic forecast and inherent uncertainty quantification, crucial for representing the range of possibilities under routine conditions.
    
    \item \textbf{Stage 2: LLM-Enhanced Contextual Adjustment (Fig.\ref{fig:1} B)}: This stage integrates real-time contextual information $\mathbf{s}$ to refine the baseline forecast, addressing the limitations of models relying solely on historical patterns. The contextual information $\mathbf{s}$, which can include structured data and unstructured text, is processed by an LLM. The LLM executes a chain-of-thought process: first summarizing the event information, then reasoning about its likely causal impact on traffic flow (location, severity, duration), and finally assessing the quantitative effect. Then the LLM selects the most plausible trajectory $\hat{\mathbf{X}}^*_{T+1:T+n}$ or potentially generates an adjusted trajectory that better reflects the anticipated impact of the event. This step leverages the LLM's ability to understand and reason about novel or complex situations described in natural language, effectively modulating the initial probabilistic forecast based on real-time context.
    
    \item \textbf{Stage 3: LLM-Powered Reporting and Decision Support (Fig.\ref{fig:1} C)}: The final stage focuses on translating the adjusted forecast $\hat{\mathbf{X}}^*_{T+1:T+n}$ into practical outputs for end-users. The LLM receives the context-adjusted forecast and potentially relevant historical traffic guidance data. This historical guidance data allows the LLM to learn implicit operational preferences and common responses implemented by human traffic controllers in similar past situations. Based on the adjusted forecast, the contextual information, and the learned operational patterns, the LLM generates: (i) a concise, human-readable summary report describing the anticipated traffic conditions, highlighting potential issues (e.g., specific bottlenecks, expected delay increases), and explaining the reasoning based on the contextual factors; and (ii) actionable recommendations for traffic management (e.g., "Consider adjusting signal timing plan B on Corridor X between 8-10 AM," "Disseminate advisory regarding lane closure on Highway Y," "Prepare diversion route Z"). This stage bridges the gap between raw numerical prediction and practical operational utility, providing explainable insights and decision support. 
\end{itemize}

\subsection{Stage 1: Foundational Probabilistic Forecasting}

In this stage, we reformulate the time-series forecasting problem as a specialized language modeling task. By mapping continuous traffic dynamics into discrete semantic tokens, we leverage the robust reasoning capabilities of the Transformer architecture to capture complex temporal dependencies and model the aleatoric uncertainty inherent in stochastic traffic flows.

\subsubsection{Sequence Serialization and Tokenization Strategy}
Unlike traditional point-wise forecasting models, we adopt a patch-based tokenization strategy inspired by the Chronos paradigm \cite{ansari_chronos_2024}. This approach reduces the sequence length complexity from $O(T^2)$ to $O((T/S)^2)$ while preserving local temporal semantics.

\paragraph{Temporal Patching}
Given a univariate time series $\mathbf{x}^i = \{x^i_1, \dots, x^i_T\} \in \mathbb{R}^T$ for a spatial node $i$, we first decompose the sequence into a series of overlapping patches. Let $P$ denote the patch length and $S$ the stride. The sequence is unfolded into a matrix of patches $\mathcal{P}^i = \{\mathbf{p}^i_1, \dots, \mathbf{p}^i_N\}$, where the $j$-th patch $\mathbf{p}^i_j \in \mathbb{R}^P$ is defined as:
\begin{equation}
    \mathbf{p}^i_j = [x^i_{(j-1)S+1}, \dots, x^i_{(j-1)S+P}]
\end{equation}
where $N = \lfloor (T-P)/S \rfloor + 1$ is the number of tokens.

\paragraph{Local Scaling for Stationarity}
Traffic data exhibits significant non-stationarity (e.g., varying peak hours across days). To ensure the distribution of values within each patch falls into a learnable range for the quantizer, we apply instance-level mean scaling. For each patch $\mathbf{p}^i_j$, we compute a local scale factor $s_j = \frac{1}{P} \sum_{k=1}^P |\mathbf{p}^i_{j,k}| + \epsilon$. The scaled patch is obtained via:
\begin{equation}
    \tilde{\mathbf{p}}^i_j = \frac{\mathbf{p}^i_j}{s_j}
\end{equation}
This normalization allows the model to learn scale-invariant temporal patterns, generalizing across different traffic volume levels.

\paragraph{Quantization and Vocabulary Mapping}
To interface with the categorical nature of language models, we map the continuous scaled domain $\mathbb{R}$ to a discrete codebook $\mathcal{C} = \{c_1, \dots, c_V\}$ of size $V$. We employ a quantization function $Q: \mathbb{R} \to \{1, \dots, V\}$ using uniform binning within a fixed range $[\text{min}, \text{max}]$. The token ID $z_{j,k}$ for the $k$-th element of the $j$-th patch is derived as:
\begin{equation}
    z_{j,k} = Q(\tilde{p}^i_{j,k}) = \text{clip}\left(\left\lfloor \frac{\tilde{p}^i_{j,k} - \text{min}}{\text{max} - \text{min}} \times (V-1) \right\rfloor, 0, V-1\right)
\end{equation}
Consequently, the continuous time series $\mathbf{x}^i$ is transformed into a sequence of discrete token IDs $\mathbf{Z}^i = \{z_{1,1}, \dots, z_{N,P}\}$, which serves as the ``sentence'' input to the Transformer.

\subsubsection{Encoder-Decoder Architecture}
We employ a modified T5 (Text-to-Text Transfer Transformer) backbone \cite{raffel2020exploring} to process the tokenized traffic sequences. The architecture consists of an encoder that maps the historical token sequence to a latent representation, and a decoder that autoregressively generates future tokens.

\paragraph{Self-Attention with Relative Position Bias}
The core mechanism is the Multi-Head Self-Attention (MHSA). Unlike standard Transformers that use absolute sinusoidal positional encodings, T5 utilizes relative positional embeddings, which is crucial for time series as it naturally models the ``time lag'' distance between patches. The attention score between query $q$ and key $k$ is computed as:
\begin{equation}
    \mathcal{A}_{q,k} = \text{softmax}\left( \frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_{model}}} + \mathbf{B}_{q,k} \right) \mathbf{V}
\end{equation}
where $\mathbf{B}_{q,k}$ is a learnable scalar bias added to the attention logit, representing the relative temporal distance between position $q$ and $k$.

\paragraph{Objective Function}
The model is trained to minimize the standard Cross-Entropy loss over the vocabulary $V$. Given the historical context $\mathbf{Z}_{<t}$, the model predicts the probability distribution of the next token $z_t$:
\begin{equation}
    \mathcal{L}_{\text{CE}} = - \sum_{t=1}^{L} \log P_{\theta}(z_t | \mathbf{Z}_{<t})
\end{equation}
We incorporate a multi-task learning objective by combining the primary forecasting task with a random masking reconstruction task (BERT-style) to enhance the model's robustness against missing data and noise.

\subsubsection{Probabilistic Trajectory Generation}
To capture the aleatoric uncertainty inherent in future traffic states, we move beyond point estimation to probabilistic trajectory generation.

\paragraph{Nucleus Sampling (Top-$p$)}
During inference, instead of greedy decoding (selecting the token with max probability), we approximate the posterior distribution $P(\mathbf{X}_{\text{future}} | \mathbf{X}_{\text{history}})$ by sampling $K$ independent hypotheses (trajectories). We employ Nucleus Sampling, which truncates the tail of the distribution, considering only the smallest set of top tokens whose cumulative probability exceeds a threshold $p$:
\begin{equation}
    \mathcal{V}^{(p)} = \{z \in \mathcal{C} \mid \sum_{z' \in \mathcal{V}^{(p)}} P_\theta(z'|z_{<t}) \geq p\}
\end{equation}
At each step $t$, the next token $z_t^{(k)}$ for the $k$-th hypothesis is sampled from the re-normalized distribution over $\mathcal{V}^{(p)}$.

\paragraph{De-tokenization and Aggregation}
The generated token sequences are mapped back to the continuous domain via inverse quantization and inverse scaling using the stored local scale factors $s_j$. Since patches overlap, the value at a specific time step $t$ is reconstructed by averaging the predictions from all patches covering that step, ensuring smoothness at patch boundaries:
\begin{equation}
    \hat{x}^{(k)}_t = \frac{1}{|\Omega_t|} \sum_{j \in \Omega_t} s_j \cdot Q^{-1}(z^{(k)}_{j, \text{idx}(t)})
\end{equation}
where $\Omega_t$ is the set of patches containing time step $t$. This process yields a set of candidate trajectories $\mathcal{H} = \{\hat{\mathbf{X}}^{(1)}, \dots, \hat{\mathbf{X}}^{(K)}\}$ representing plausible future scenarios.

\subsection{Stage 2: Logic-Enhanced Contextual Trajectory Selection}
While Stage 1 provides a robust probabilistic baseline based on historical patterns, it lacks the semantic understanding to account for varying external disruptions (e.g., accidents, extreme weather). Stage 2 bridges this gap by employing a Large Language Model (LLM) as a logic-driven reasoner to evaluate and select the most plausible trajectory $\hat{\mathbf{X}}^*$ from the candidate set $\mathcal{H}$ based on real-time context $\mathbf{s}$.

\subsubsection{Semantic Serialization of Probabilistic Forecasts}
Since LLMs operate in the semantic space rather than the numerical tensor space, we must project the candidate set $\mathcal{H} = \{\hat{\mathbf{X}}^{(1)}, \dots, \hat{\mathbf{X}}^{(K)}\}$ into a textual representation compatible with the LLM's context window.
We define a serialization function $\psi: \mathbb{R}^{n} \to \mathcal{S}$ that aggregates high-dimensional trajectory data into descriptive statistics. For each candidate $k$, we compute the statistical summary vector $\mathbf{v}^{(k)}$ over key corridors, including the minimum, median, and maximum speeds across the prediction horizon. This is formatted into a structured prompt segment:
\begin{equation}
    \mathcal{T}_{traj}^{(k)} = \text{``Candidate } k: \text{Trend } \in [\min(\mathbf{v}^{(k)}), \max(\mathbf{v}^{(k)})], \text{Dynamics: } \text{Description}(\nabla \mathbf{v}^{(k)}) \text{''}
\end{equation}
where $\text{Description}(\cdot)$ maps numerical gradients to linguistic descriptors (e.g., ``rapidly decaying,'' ``stable''). This allows the LLM to comprehend the traffic dynamics implied by each hypothesis without processing raw tensors.

\subsubsection{Chain-of-Thought Reasoning and Selection}
We leverage the reasoning capabilities of the Qwen2.5-14B-Instruct model \cite{qwen} to model the causal relationship between the context $\mathbf{s}$ and traffic states. The inference process is structured as a conditional probability maximization problem via Chain-of-Thought (CoT) prompting.

Let $\mathcal{P}_{sys}$ be the system instruction and $\mathcal{T}_{ctx}$ be the textualized real-time context (e.g., incident logs, weather reports). The LLM generates a reasoning path $\mathcal{R}$ followed by a selection index $k^*$:
\begin{equation}
    (\mathcal{R}, k^*) \sim P_{LLM}(\cdot \mid \mathcal{P}_{sys}, \mathcal{T}_{ctx}, \{\mathcal{T}_{traj}^{(k)}\}_{k=1}^K)
\end{equation}

The reasoning path $\mathcal{R}$ explicitly decomposes the task into three logical steps:
\begin{enumerate}
    \item \textbf{Event Impact Analysis:} The LLM parses $\mathcal{T}_{ctx}$ to extract event attributes (severity, location) and infers the spatiotemporal scope of the impact (e.g., ``Lane closure on Highway A will cause upstream congestion propagation'').
    \item \textbf{Hypothesis Verification:} The model compares the inferred impact against the statistical properties of each candidate $\mathcal{T}_{traj}^{(k)}$. For instance, if the event implies a significant speed drop, candidates showing ``stable high speed'' are rejected.
    \item \textbf{Optimal Selection:} The index $k^*$ corresponding to the candidate that maximizes semantic alignment with the reasoning $\mathcal{R}$ is selected.
\end{enumerate}
The final output is the specific trajectory $\hat{\mathbf{X}}^* = \hat{\mathbf{X}}^{(k^*)}$, which represents the event-conditioned forecast.

\subsection{Stage 3: LLM-Powered Reporting and Decision Suppor}

The objective of Stage~3 is to transform the selected traffic forecast $\hat{\mathbf{X}}^*$ into structured, actionable outputs by incorporating historical traffic management experience. Rather than relying solely on generative inference, this stage conditions the generation process on retrieved historical cases, thereby grounding the outputs in previously observed traffic contexts and response patterns.

\subsubsection{Historical Case Retrieval}

We construct a historical case repository $\mathcal{K} = \{(c_m, a_m)\}_{m=1}^M$, where each entry consists of a past traffic context $c_m$ (e.g., incident type and observed traffic state) and the corresponding management action $a_m$ (e.g., signal control strategies or information dissemination measures). This repository serves as a source of empirical reference for the current scenario.

Given the current system state $\mathbf{s}$ and the selected forecast summary $\hat{\mathbf{X}}^*$, we form a joint query representation $q$ by concatenation. A dense retrieval model is then employed to identify historical cases that are semantically similar to the current situation. Specifically, cosine similarity is computed between the query embedding $\phi(q)$ and the stored context embeddings $\phi(c_m)$:
\begin{equation}
    \mathcal{S}_{rel} = \{ (c_m, a_m) \mid \cos(\phi(q), \phi(c_m)) > \tau \},
\end{equation}
where $\phi(\cdot)$ denotes a pre-trained sentence encoder and $\tau$ is a similarity threshold. The top-$N$ retrieved cases constitute the reference set $\mathcal{S}_{rel}$, which provides contextual constraints for the subsequent generation stage.

\subsubsection{Structured Output Generation}

The final output is produced by a conditional generator $G$, which integrates the traffic forecast, the current contextual description, and the retrieved historical cases:
\begin{equation}
    \mathbf{Y} = G(\hat{\mathbf{X}}^*, \mathcal{T}_{ctx}, \mathcal{S}_{rel}).
\end{equation}
The generated output $\mathbf{Y}$ is organized into two complementary components:
\begin{itemize}
    \item \textit{Situation Summary} ($\mathbf{Y}_{rep}$): a concise description of the anticipated traffic evolution derived from $\hat{\mathbf{X}}^*$, highlighting key temporal and spatial characteristics relevant to traffic management.
    \item \textit{Action Suggestions} ($\mathbf{Y}_{rec}$): a set of recommended response measures informed by the retrieved cases in $\mathcal{S}_{rel}$ and adapted to the current forecasted conditions.
\end{itemize}

By conditioning the generation process on historically similar traffic scenarios, this framework encourages consistency with prior management patterns while allowing flexibility to accommodate the specific characteristics of the current forecast.


\section{Experiments \& Results}
To rigorously evaluate the efficacy of the Chat-ITS framework, we present a comprehensive experimental analysis. We first detail the experimental setup—including datasets, implementation details, baselines, and metrics—in Section \ref{sec:setup}. Subsequently, we report the empirical results under routine traffic conditions (Section \ref{sec:routine}), during anomalous events (Section \ref{sec:anomaly}), and analyze the system's explainability (Section \ref{sec:explainability}) and component contributions (Section \ref{sec:ablation}).

\subsection{Materials and Methods}
\label{sec:setup}

\subsubsection{Datasets and Preprocessing}
This study utilizes multiple large-scale, real-world traffic and contextual datasets collected by Amap across China, primarily focusing on Beijing for foundational model training and broader regions for event context and specific analyses. All datasets cover the period from September 2023 to May 2024, unless otherwise specified.

\textbf{Spatio-Temporal Traffic Data:} The core dataset for training the probabilistic foundation model consists of high-resolution traffic state information for the urban core of Beijing. Raw traffic data was aggregated onto a regular grid with a spatial resolution of $500m \times 500m$. This resulted in $N=5,797$ distinct spatial grid cells covering the main urban road network. For each cell, key traffic state variables, including traffic volume (vehicles per interval) and average speed (km/h), were computed and aggregated into 5-minute intervals. This dataset comprises approximately 1.3 billion data points, providing a comprehensive representation of urban traffic dynamics much larger than many commonly used open-source benchmarks \cite{liu_largest_2023}.

\textbf{Venue-Centric Traffic Data:} To specifically capture traffic patterns influenced by large-scale public events, supplementary datasets focused on major venues were compiled.
\begin{itemize}
    \item \textit{Aggregated Venue Flow:} Derived from location-based service data, aggregated traffic volume information was obtained for the precise geographical boundaries and surrounding buffer areas of over 300 major venues (sports stadiums, concert halls, major tourist attractions, transport hubs) across multiple cities in China. This dataset helps model event-specific demand surges and dispersion patterns.
    \item \textit{Fine-Grained Venue Grid Data:} For a subset of the venues above, traffic volume was aggregated onto a finer $100m \times 100m$ grid covering the venue. Due to the high granularity leading to sparsity, we identified and utilized data primarily from the top-20 grid cells exhibiting the highest average historical traffic volume within each venue's defined area, focusing analysis on the most relevant micro-locations.
\end{itemize}

\textbf{Contextual Event Data:} Real-time and historical event information, crucial for the LLM reasoning stage (Stage 2) and evaluation during anomalies, was primarily sourced from the Amap open platform APIs and associated historical logs for the relevant periods and geographical areas. This included:
\begin{itemize}
    \item \textit{Structured Construction Data:} A curated dataset encompassing over 10,000 construction events on highways and major arterials. Each entry typically includes precise location information (coordinates or road segment identifiers), scheduled start and end times, number and type of lanes affected (e.g., closure, partial blockage), and nature of the work.
    \item \textit{Unstructured Anomaly Reports:} Traffic incident information disseminated via Amap, originating from user reports or official traffic authority alerts. These reports typically contain a natural language description of the incident (e.g., "Accident involving two cars on Ring Road eastbound near Exit 5, blocking right lane"), an approximate location , and a timestamp. This text serves as direct input for the LLM.
\end{itemize}

\textbf{Preprocessing:} Standard preprocessing steps were applied to the traffic datasets before model training and evaluation. Missing values in the time-series data less than 5\% of points were imputed using linear interpolation while others are dropped. Traffic state features (speed, volume) were normalized using Z-score normalization based on the mean and standard deviation calculated from the training portion of the primary Beijing dataset. For GNN-based baselines, the spatial graph adjacency matrix was constructed based on road network distance threshold, with edge weights typically defined by inverse distance. Unstructured text data from anomaly reports and event information was cleaned to remove irrelevant artifacts before being fed into the LLM prompts.

\subsubsection{Baseline Implementations}
The chosen baselines represent a broad spectrum of modern time series forecasting methodologies, ensuring a robust and comprehensive comparison against different architectures:

\begin{itemize}
    \item DLinear\cite{zeng_are_2022}: Represents simple yet surprisingly effective linear models, serving as a strong benchmark against more complex architectures by decomposing the time series and applying separate linear layers. It challenges the necessity of intricate designs for certain forecasting tasks.
    \item FiLM \cite{zhou2022film}: Represents linear models enhanced with frequency analysis, designed to improve forecasting by better capturing periodicity through specific decomposition techniques applied in the frequency domain.
    \item Informer \cite{zhou2021informer}: A prominent Transformer-based model optimized for long sequence time-series forecasting (LSTF) efficiency through a ProbSparse self-attention mechanism and distilling operation, representing efficient Transformer variants.
    \item PatchTST \cite{nie_time_2023}: Represents channel-independent Transformer approaches utilizing patching, where input time series are divided into subseries-level patches that are fed as tokens to the Transformer, capturing local semantic information.
    \item Chronos \cite{ansari_chronos_2024}: Represents recent large pre-trained foundation models for time series, leveraging language model architectures scaled to time series data for zero-shot or few-shot forecasting, showcasing the potential of large-scale pre-training.
    \item iTransformer \cite{liu_itransformer_2023}: An innovative Transformer architecture that inverts the standard process by applying attention to embedded variates across the entire time series length, designed to better capture multivariate correlations.
\end{itemize}

For implementation, we utilized established frameworks such as TSLib \cite{wu2023timesnet, wang2024tssurvey} or the official public code repositories associated with each baseline model. To guarantee a fair and direct comparison, all baseline models were trained using the identical historical dataset that was employed for training the Chat-ITS foundation model. The sole exception was Chronos, for which we leveraged the publicly available pre-trained weights, applying it directly as a zero-shot forecaster without fine-tuning on our specific dataset.

\subsubsection{Evaluation Details}
We evaluated the model's performance using standard time-series forecasting metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and Weighted Absolute Percentage Error (WAPE). MAE measures the average absolute difference between predictions and actual values. MSE computes the average of the squared errors, which emphasizes larger deviations more heavily. RMSE is the square root of MSE, bringing the error back to the original scale of the data. MAPE and WAPE assess the relative size of errors compared to actual values, providing scale-independent evaluation.

It is worth noting that we preferred WAPE over MAPE for traffic volume forecasting tasks, where the data often contains zero or near-zero values. In such scenarios, MAPE can become undefined or unstable due to division by zero or extremely small actual values, leading to misleading evaluations. Additionally, in cases with large variance in traffic volumes, MAPE tends to overemphasize errors in low-volume periods while underrepresenting high-volume ones. In contrast, WAPE normalizes total absolute error by the sum of actual values across all time steps and locations, offering a more stable and representative metric under these conditions.

The specific metrics are defined as \eqref{mae}, \eqref{mse}, \eqref{rmse}, \eqref{mape}, and \eqref{wape}:

\begin{equation} \label{mae}
\text{MAE} = \frac{1}{nN} \sum_{t=T+1}^{T+n} \sum_{i=1}^{N} |x_{t,i} - \hat{x}_{t,i}|
\end{equation}
\begin{equation} \label{mse}
\text{MSE} = \frac{1}{nN} \sum_{t=T+1}^{T+n} \sum_{i=1}^{N} (x_{t,i} - \hat{x}_{t,i})^2
\end{equation}
\begin{equation} \label{rmse}
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{ \frac{1}{nN} \sum_{t=T+1}^{T+n} \sum_{i=1}^{N} (x_{t,i} - \hat{x}_{t,i})^2 }
\end{equation}
\begin{equation} \label{mape}
\text{MAPE} = \frac{1}{nN} \sum_{t=T+1}^{T+n} \sum_{i=1}^{N} \left| \frac{x_{t,i} - \hat{x}_{t,i}}{x_{t,i}} \right|
\end{equation}
\begin{equation} \label{wape}
\text{WAPE} = \frac{\sum_{t=T+1}^{T+n} \sum_{i=1}^{N} |x_{t,i} - \hat{x}_{t,i}|}{\sum_{t=T+1}^{T+n} \sum_{i=1}^{N} |x_{t,i}|}
\end{equation}

\textbf{Anomaly Identification}: For evaluating performance during anomalies, event periods were identified using timestamps from the Amap/Gaode construction and incident logs.  Anomalous periods were defined as 1 hour before to 1 hours after the logged event time for relevant locations. For public events, the anomalous period covered 2 hours before the event start to 2 hours after the event end. Zero-shot evaluation used events from categories completely held out during any training/fine-tuning. 

\textbf{Data Splits}: Data was split chronologically for each city/dataset. Typically, the first 70\% was used for pre-training the foundation model, the next 10\% for validation, and the final 20\% for testing.

\subsection{Baseline Performance under Routine Conditions}
\label{sec:routine}

To establish the foundational capability of our framework, we evaluated the performance of the pre-trained spatio-temporal model (Stage 1 output) under routine traffic conditions, comparing it against established baselines. The evaluation used Beijing datasets (Jan-May 2024), excluding major anomalies.

Our results, summarized in Table \ref{table:1}, demonstrate that the Chat-ITS foundational model substantially outperforms all evaluated deep learning baselines under these normal conditions. Across all prediction horizons (15, 30, and 60 minutes) and all metrics (MAE, MSE, WAPE), our model consistently achieved the lowest error rates. Notably, while Informer emerged as the most competitive baseline, our model still surpassed its performance considerably. For instance, the average MAE for our model (0.153) was markedly lower than Informer's (0.166) and substantially better than PatchTST (0.259) or FiLM (0.527).

Furthermore, our model displayed remarkable stability across prediction horizons, maintaining consistently low error values even for 60-minute forecasts. This contrasts with baselines like DLinear and PatchTST, which exhibited pronounced accuracy degradation as the horizon increased. This confirms that the foundation model provides a robust starting point for subsequent contextual adjustment.

\input{tables/table1}

\subsection{Enhanced Prediction Accuracy across Diverse Anomalous Events}
\label{sec:anomaly}

We evaluate the effectiveness of incorporating textual context into traffic forecasting under four representative anomalous scenarios, covering both routine disruptions and semantically ambiguous events. The proposed Chat-ITS model is compared with a foundation-model baseline that relies solely on historical traffic observations. Figure~\ref{fig:2} shows that, when exposed to anomalous inputs, the baseline frequently produces predictions with large uncertainty, whereas Chat-ITS yields more concentrated and accurate forecasts.

Figure~\ref{fig:2} presents illustrative examples across different event types. In common disruption scenarios such as highway construction and traffic incidents (Fig.~\ref{fig:2}(a) and Fig.~\ref{fig:2}(b)), Chat-ITS accurately captures the expected reductions in traffic volume or speed and closely follows the ground-truth trajectories. In contrast, the baseline model exhibits substantial predictive uncertainty and struggles to estimate the severity of the impact.

More pronounced differences are observed in semantically nuanced scenarios. In the parking control case (Fig.~\ref{fig:2}(c)), the baseline extrapolates historical stability and fails to anticipate a complete stop, resulting in a wide predictive interval. By incorporating event descriptions related to destination adjustment, Chat-ITS correctly forecasts a sharp speed reduction approaching zero. Similarly, in the accident with rerouting scenario (Fig.~\ref{fig:2}(d)), the baseline prediction reflects a strong prior association between accidents and congestion, leading to lower-speed estimates. Chat-ITS, however, accounts for the rerouting information and predicts sustained high-speed flow, consistent with the observed outcome.

Overall, these results indicate that integrating event-level textual context enables the model to disambiguate anomalous conditions and to revise statistically dominant patterns when additional semantic information is available.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figure2.pdf}
\caption{Time-series predictions under different anomalous traffic scenarios.
Results are shown for (a) highway construction, (b) traffic incident, (c) parking control, and (d) accident with rerouting.
The blue shaded region denotes the predictive uncertainty of the baseline model without textual input.
Predictions from Chat-ITS are shown as an orange dashed line, while the ground truth is shown as a black solid line.}
\label{fig:2}
\end{figure*}

\input{tables/table2}

\subsection{Explainable Reporting and Actionable Recommendations}
\label{sec:explainability}

Stage 3 of the framework utilizes the LLM to translate quantitative forecasts into human-readable reports. Figure \ref{fig:reports_recommendations} showcases this capability:
\begin{itemize}
  \item \textbf{Routine Scenario:} The LLM provides concise confirmation ("Expect typical heavy congestion") and prompts standard checks ("Ensure ramp metering active"), building trust.
  \item \textbf{Construction Event:} The output is analytical, synthesizing the speed drop with the "single lane closure" context. Recommendations are proactive, suggesting "deploying VMS advisory" and "adjusting signal timings" on alternate routes, demonstrating reasoning about network-wide ripple effects.
  \item \textbf{Public Event:} The report highlights specific egress congestion timing. Recommendations like "Monitor parking lot clearance rates" are highly tailored to the event type.
\end{itemize}
These outputs demonstrate Chat-ITS's capacity to bridge the gap between prediction and action, transforming numerical data into operationally relevant intelligence.

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{fig3.pdf}
  \caption{\textbf{LLM-generated explainable reports and actionable recommendations.} Examples regarding (a) Routine Congestion, (b) Construction Event, and (c) Public Event Egress, showing the translation from context-aware prediction to operational intelligence.} \label{fig:reports_recommendations}
\end{figure}

\subsection{Ablation Studies}
\label{sec:ablation}

We performed ablation studies on the anomaly dataset to validate component contributions:
\begin{enumerate}
    \item \textbf{Necessity of LLM Adjustment:} Removing Stage 2 and relying solely on the foundation model resulted in noticeable accuracy degradation during anomalies (Figure \ref{fig:ablation_studies}a), confirming the essential role of contextual reasoning.
    \item \textbf{Probabilistic vs. Deterministic Input:} Providing the LLM with multiple probabilistic candidate trajectories (Stage 1 output) yielded better performance than providing a single deterministic mean forecast (Figure \ref{fig:ablation_studies}b). This suggests the probabilistic distribution offers a richer search space for the LLM to select the most contextually plausible outcome.
    \item \textbf{Context Quality:} While performance dropped when input context was intentionally degraded (vague/incorrect), Chat-ITS still outperformed context-unaware baselines, indicating a degree of robustness (Figure \ref{fig:ablation_studies}c).
\end{enumerate}

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{ablation_studies_refined_horizontal_xticks.pdf}
  \caption{\textbf{Ablation study results.} (a) Impact of removing LLM adjustment. (b) Benefit of probabilistic candidates vs. deterministic input. (c) Sensitivity to context quality.} \label{fig:ablation_studies}
\end{figure}

\section{Discussion}
In this work, we introduced Chat-ITS, a hybrid framework that synergistically combines a pre-trained spatio-temporal foundation model for probabilistic forecasting with the contextual reasoning capabilities of Large Language Models to address key limitations in current traffic prediction systems. Our results demonstrate that Chat-ITS achieves forecasting accuracy comparable to state-of-the-art deep learning methods under routine traffic conditions. More importantly, it marked outperforms these baselines during anomalous events, such as construction, incidents, and public gatherings, by effectively incorporating real-time structured and unstructured contextual information via LLM processing. We showed substantial error reductions (up to 15\%) during such events, highlighting the framework's enhanced adaptability and resilience. Furthermore, case studies illustrated Chat-ITS's promising zero-shot generalization capability, allowing it to interpret and respond to novel event types described solely through natural language prompts. Finally, we demonstrated the framework's ability to generate explainable, human-readable summary reports and actionable traffic management recommendations, bridging the critical gap between prediction and operational decision-making.

The effectiveness of Chat-ITS stems from its deliberate hybrid design, which leverages the complementary strengths of deep learning for pattern recognition in high-dimensional spatio-temporal data and LLMs for flexible context understanding, reasoning, and natural language generation. The foundation model provides a statistically robust baseline forecast capturing complex recurring dynamics and uncertainty. The LLM, instead of being burdened with direct numerical prediction, focuses on its core strengths: interpreting diverse inputs (text, structured data), inferring causal impacts of events, evaluating scenarios based on context, and communicating findings effectively. This division of labor allows Chat-ITS to overcome the brittleness of purely data-driven models during anomalies and the limitations of purely LLM-based approaches in precise numerical forecasting. The integration of historical dispatch patterns further enhances the practical relevance of the generated recommendations. This synergistic approach represents a noticeable step towards ITS that are not only predictive but also adaptive, explainable, and operationally relevant.

The uniqueness of Chat-ITS lies in its synergistic integration of two powerful AI paradigms: deep learning for robust forecasting and LLMs for contextual reasoning. While deep learning models have pushed the boundaries of accuracy on benchmark datasets, they often lack robustness to out-of-distribution events and fail to provide actionable insights. Attempts to incorporate auxiliary data often rely on rigid input structures and struggle with unstructured text. LLM applications in transportation have primarily focused on tasks like route planning, dialogue systems, or summarizing traffic reports, but their direct application to end-to-end numerical forecasting remains challenging. Chat-ITS uniquely integrates these two powerful AI paradigms in a way that mitigates their respective weaknesses while harnessing their combined potential for context-aware, explainable, and actionable traffic prediction.

The performance of Chat-ITS during anomalies is inherently dependent on the availability and quality of real-time contextual information ($\mathbf{s}$). Inaccurate or delayed event reports will naturally limit the effectiveness of the LLM adjustment stage. While we demonstrated some robustness, further research is needed on handling noisy or conflicting contextual inputs. The reliance on LLMs also introduces computational costs associated with inference, although using smaller or optimized LLMs could mitigate this, and exploring different LLM architectures, including potentially smaller, domain-adapted models, could optimize this trade-off. Potential issues related to LLM biases or hallucinations, while mitigated by grounding the LLM's task in evaluating pre-generated trajectories, require ongoing vigilance and potentially safety layers in operational deployment. Furthermore, effective prompt engineering is crucial for eliciting the desired reasoning and output from the LLM, which may require domain expertise. Scalability to extremely large, city-wide networks with tens of thousands of sensors also needs further investigation.

Future research will focus on integrating Chat-ITS with real-time traffic control systems (e.g., adaptive signal control, variable speed limits) to enable fully automated, context-aware traffic management. We also aim to incorporate a wider range of contextual data sources, such as real-time social media feeds, advanced weather nowcasting, or connected vehicle data, to further enhance situational awareness. Developing more sophisticated methods for the LLM to not just select but actively modify forecast trajectories based on context could yield further accuracy gains. Finally, conducting user studies with traffic operators to evaluate the usability and effectiveness of the generated reports and recommendations in real-world control room settings is essential for practical validation and refinement.

\section*{Acknowledgments}

This work was supported by Beijing Natural Science Foundation (No. JQ24051), Beijing Nova Program (No. 20230484432) and Independent Research Project of the State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University (No. ZZ-GG-20250406).

\section{Data Availability}

The datasets central to this study, including the large-scale traffic network data and the anomalous event logs, were provided by Amap. Due to the proprietary nature of this information, which encompasses commercial sensitivities and privacy considerations, these datasets are not publicly available. We acknowledge the importance of reproducibility and regret that these necessary restrictions prevent the public dissemination of these materials. Enquiries regarding the methodology or potential collaborations may be directed to the corresponding author.

\section{Supplementary Materials}
\begin{itemize}
    \item \textbf{Text S1 and Table S1.} Detailed description and illustrative examples of the Structured Construction Data used in this study.

    \item \textbf{Text S2 and Table S2.} Detailed description and examples of the Unstructured Anomaly Reports.
\end{itemize}

\printbibliography

\end{document}