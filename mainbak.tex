\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{setspace}
\usepackage[margin=1.25in]{geometry}
\usepackage{graphicx}
\graphicspath{ {./figures/} }
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lineno}
\usepackage{booktabs}
\usepackage{rotating}
\linenumbers


%%%%%% Bibliography %%%%%%
% Replace "sample" in the \addbibresource line below with the name of your .bib file.
\usepackage[style=nejm, 
citestyle=numeric-comp,
sorting=none]{biblatex}
\addbibresource{ref.bib}

%%%%%% Title %%%%%%
% Full titles can be a maximum of 200 characters, including spaces. 
% Title Format: Use title case, capitalizing the first letter of each word, except for certain small words, such as articles and short prepositions
\title{Harnessing Large Language Models for Adaptive and Explainable Traffic Forecasting}

%%%%%% Authors %%%%%%
% Authors should be listed in order of contribution to the paper, by full first name, then middle initial (if any), followed by last name and separated by commas.
% Please do not use initials for first names. If you use your middle name as a full name, use an initial for the first name and spell out your full middle name.
% Use a superscript asterisk (*) to identify the corresponding author and be sure to include that person’s e-mail address. Use symbols (in this order: †, ‡, §, ||, ¶, #, ††, ‡‡, etc.) for author notes, such as present addresses, “These authors contributed equally to this work” notations, and similar information.
% You can include group authors, but please include a list of the actual authors (the group members) in the Supplementary Materials.
\author[1]{Haoyang Yan}
\author[1,2]{Xiaolei Ma*}
\author[3]{Hao Frank Yang}
\author[4,5]{Ziyuan Pu}
\author[6]{Yi Li}
\author[7]{Yuelong Su}

%%%%%% Affiliations %%%%%%
\affil[1]{School of Transportation Science and Engineering, Beihang University, Beijing, China.}
\affil[2]{Key Laboratory of Intelligent Transportation Technology and System of the Ministry of Education, Beihang University, Beijing 102206, China}
\affil[3]{Department of Civil and System Engineering, Johns Hopkins Data Science \& AI Institute, Whiting School Engineering, Johns Hopkins University, Baltimore, MD 21218}
\affil[4]{School of Transportation, Southeast University, Nanjing, 211189, China}
\affil[5]{Jiangsu Province Collaborative Innovation Center of Modern Urban Traffic Technologies, Southeast University, Nanjing, 211189, China}
\affil[6]{AutoNavi Software Company, Ltd, Beijing 100020, China}
\affil[7]{State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University, Beijing, 100084, PR China}
\affil[*]{Address correspondence to: xiaolei@buaa.edu.cn}
% \affil[$\dag$]{These authors contributed equally to this work.}

%%%%%% Date %%%%%%
% Date is optional
\date{}

%%%%%% Spacing %%%%%%
% Use paragraph spacing of 1.5 or 2 (for double spacing, use command \doublespacing)
\onehalfspacing

\begin{document}

\maketitle

%%%%%% Abstract %%%%%%
\begin{abstract}
Accurate traffic prediction is fundamental for Intelligent Transportation Systems (ITS) aiming to alleviate congestion and improve urban mobility resilience. While deep learning approaches like graph neural networks and sequence models have advanced short-term forecasting accuracy under standard conditions, their practical deployment is often hampered by noticeable limitations. These models typically struggle to generalize during anomalous events such as road incidents or severe weather, exhibit inflexibility in incorporating diverse real-time contextual information, and produce outputs that lack the interpretability and actionable insights crucial for operational traffic management. Separately, large language models (LLMs), despite their reasoning power, face inherent challenges in direct numerical time-series prediction and require substantial resources for task-specific fine-tuning, limiting their standalone applicability. Here, we introduce Chat-ITS, a novel hybrid framework designed to overcome these challenges by synergistically combining robust probabilistic time-series forecasting with the contextual reasoning capabilities of LLMs. Chat-ITS first generates multiple candidate traffic state trajectories and associated uncertainty bounds using a specialized probabilistic forecaster. Subsequently, an LLM processes these candidates, conditioned on flexible natural language prompts that encode both structured data (e.g., weather forecasts, road closures) and unstructured information (e.g., descriptions of public events). The LLM selects the most contextually plausible trajectory and, critically, generates human-readable explanations and actionable recommendations for traffic operators. We demonstrate through extensive evaluation under both routine and diverse anomalous scenarios that Chat-ITS enhances prediction accuracy during irregular events compared to baseline models, while maintaining state-of-the-art performance under normal traffic conditions. Furthermore, case studies highlight the framework's ability to handle novel events described only through text prompts by leveraging the LLM's reasoning to select the most plausible outcome from a range of probabilistically generated forecasts; this provides context-aware, actionable insights (e.g., suggesting specific signal timing adjustments or dynamic routing strategies), thereby bridging the gap towards more adaptive, effective, and practical ITS applications.
\end{abstract}

%%%%%% Main Text %%%%%%

\section{Introduction}

Accurate traffic prediction is fundamental to the efficacy of Intelligent Transportation Systems (ITS), enabling critical functions such as dynamic route guidance, adaptive traffic signal control, and proactive incident management essential for mitigating congestion, reducing emissions, and enhancing urban mobility resilience \cite{wu_big-data_2025}. Congestion alone costs economies billions annually and degrades quality of life in urban centers \cite{avila_data-driven_2020}. Effective ITS, powered by reliable forecasts, promises substantial improvements in transportation efficiency and sustainability. Recent advances, particularly the application of deep learning techniques like graph neural networks (GNNs) for modeling complex spatial dependencies across road networks \cite{shao_pre-training_2022} and sophisticated sequence models (e.g., temporal convolution networks, attention mechanisms) for capturing temporal dynamics \cite{li_transferable_2024, runge_inferring_2019}, have considerable improved short-term forecasting accuracy under typical, recurring traffic conditions \cite{yuan_unist_2024}. These methods effectively learn patterns from large historical datasets, providing a strong foundation for next-generation ITS applications operating under predictable circumstances.

Despite these successes, existing state-of-the-art traffic forecasting methods face critical limitations that hinder their real-world operational utility, particularly under non-routine circumstances \cite{guo_towards_2024, williams_context_2024, liu_time-mmd_2024}. Firstly, their predictive performance often degrades sharply during anomalous events such as road accidents, unexpected road closures, severe weather conditions, or large-scale public gatherings \cite{li_language_2025, xue_promptcast_2023}. Models trained primarily on routine historical patterns often exhibit poor generalization capabilities when faced with data distributions shifted by these irregular occurrences \cite{arango_chronosx_2025, zeng_are_2022}. This fragility undermines their reliability precisely when accurate prediction is most needed for effective incident response and management. Secondly, the fixed input encoding mechanisms of many deep learning models limit their ability to flexibly incorporate diverse, unstructured, or dynamic updates on road work schedules often contains crucial context for anticipating traffic impacts. Integrating textual incident reports, event schedules, social media alerts, or unforeseen disruptions often requires complex feature engineering or extensive model retraining, impeding adaptation to unforeseen event types without clear overhead \cite{tan_are_2024}. Thirdly, and perhaps most crucially for translation into practice, the standard output of these models, typically a high-dimensional matrix or tensor representing predicted speeds or flows, lacks direct interpretability. It fails to convey the underlying reasons for the predicted state or provide actionable guidance for traffic operators and decision-makers \cite{yuan_diffusion-ts_2023}. Consequently, even statistically accurate forecasts may not readily translate into effective, timely, and context-aware traffic management interventions, limiting the practical impact of these advanced techniques.

Large language models (LLMs) have emerged as powerful tools demonstrating remarkable capabilities in natural language understanding, contextual reasoning, and generalization across diverse tasks \cite{su_large_2024}. Their potential to process unstructured text, synthesize information from multiple sources, and generate human-like explanations offers promising avenues to address the challenges of context integration and interpretability in ITS \cite{wang_where_2023, guo_towards_2024, feng_citygpt_2024}. However, applying LLMs directly to the task of numerical time-series forecasting presents inherent difficulties. Their architectures, primarily optimized for sequential token generation, often struggle with the precise numerical regression required for traffic state prediction and can be inefficient in capturing the complex spatio-temporal statistical dependencies inherent in traffic flow \cite{tan_are_2024, liu_taming_2024}. Furthermore, training or even fine-tuning large LLMs for specialized forecasting tasks demands substantial computational resources and large-scale, domain-specific datasets, often proving impractical for widespread deployment in operational ITS settings where data characteristics can vary across locations and time \cite{jin_time-llm_2023}.

Here, we introduce Chat-ITS, a novel hybrid forecasting framework designed to bridge the gap between robust probabilistic time-series modeling and the contextual reasoning capabilities of LLMs, thereby overcoming the aforementioned limitations. Chat-ITS employs a synergistic, multi-stage approach that deliberately leverages the distinct strengths of each component. It first utilizes a dedicated spatio-temporal foundation model, pre-trained on extensive historical traffic data, to generate multiple candidate traffic state trajectories along with associated uncertainty estimates. This ensures statistical rigor and captures complex baseline traffic dynamics. Subsequently, an LLM, operating on these candidate trajectories, is conditioned on flexible natural language prompts. These prompts can seamlessly encode both structured data (e.g., quantitative weather forecasts, road closure notices with coordinates and times) and unstructured descriptions rich with linguistic cues (e.g., "Event update: sold-out show at the downtown arena, scheduled to end at 10 PM" or "Dispatch log: report of a multi-vehicle collision with emergency services responding on the northbound lane near exit 15"). The LLM evaluates the candidate trajectories within this broader context, reasoning about the likely impacts to select or adjust towards the most plausible outcome given the real-time information. Crucially, the LLM also generates human-readable explanations for its choice and actionable recommendations tailored for traffic management personnel, integrating insights potentially learned from historical operational data. This architecture deliberately avoids tasking the LLM with direct numerical prediction, instead harnessing its strengths in semantic comprehension, causal inference, and context-aware reasoning.

We demonstrate through comprehensive experiments encompassing both routine traffic patterns and a diverse set of simulated and real-world anomalous scenarios (including construction, accidents, and public events) that Chat-ITS noticeable outperforms conventional deep learning baseline models during irregular events, reducing prediction errors by up to 15\% under certain conditions, while matching state-of-the-art accuracy under normal conditions. Crucially, case studies highlight the framework's ability to generalize zero-shot to unseen event types described only via text prompts and deliver context-aware, actionable insights (e.g., suggesting specific signal timing adjustments, disseminating targeted traveler advisories, or recommending dynamic routing strategies). By integrating the statistical power of probabilistic forecasting with the semantic understanding and reasoning capabilities of language-based AI, Chat-ITS presents a new paradigm for traffic prediction, one that is not only accurate and adaptive but also explainable and directly aligned with the practical needs of transportation practitioners for effective real-world ITS deployment.

\section{Results}
\subsection{Overall Pipeline}
Traffic prediction is typically framed as a short-term time-series forecasting task, where future values $\mathbf{X}_{T+1:T+n}$ are predicted based on historical observations $\mathbf{X}_{1:T}$. This paper tackles a multi-modal version of this problem, recognizing that real-world traffic dynamics are influenced not only by past traffic states but also by a plethora of contextual factors often conveyed through textual or structured non-time-series data. We work with input instances $(\mathbf{X}_{1:T}, \mathbf{s})$, consisting of historical time series data $\mathbf{X}_{1:T} = \{\mathbf{x}_1, \ldots, \mathbf{x}_T\}$, where each $\mathbf{x}_t \in \mathbb{R}^N$ captures $D$ features of traffic states (e.g., speed, flow, occupancy) for $N$ spatial locations (e.g., road segments, sensors) over $T$ historical time steps, and auxiliary contextual information $\mathbf{s}$. This contextual information $\mathbf{s}$ can be diverse, including structured data (e.g., weather parameters, event schedules, road work logs) and unstructured natural language text (e.g., incident reports, social media alerts, news feeds) that potentially influences the time series and provides valuable context for improving forecast accuracy, especially during non-routine conditions. Our objective is to develop a model $\mathcal{F}$ that these multi-modal inputs to accurate and reliable predictions of future traffic states, potentially including uncertainty quantification. This is formalized as:
\begin{equation} \label{eq:1}
    \mathbf{X}_{T+1:T+n} = \{\mathbf{x}_{T+1}, \mathbf{x}_{T+2}, \ldots, \mathbf{x}_{T+n}\} = \mathcal{F}(\mathbf{X}_{1:T}, \mathbf{s}),
\end{equation}
where $\mathbf{X}_{T+1:T+n}$ is the predicted sequence of $n$ future state vectors or distributions. The ultimate goal is to identify an optimal model $\mathcal{F}$ that delivers accurate and reliable predictions while also being explainable and effectively leveraging the contextual information from $\mathbf{s}$ to adapt to both routine and non-routine conditions.

The Chat-ITS framework, depicted schematically in Fig.\ref{fig:1},operates through three synergistic core stages designed to integrate the strengths of advanced time-series modeling and large language models: (1) Foundational Probabilistic Forecasting, (2) LLM-Enhanced Contextual Adjustment, and (3) LLM-Powered Reporting and Decision Support.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{overview.pdf}
    \caption{Overall architecture of the Chat-ITS framework. (A) Stage 1: A pre-trained spatio-temporal foundation model processes historical traffic data $\mathbf{X}_{1:T}$ to generate multiple candidate future trajectories $\{\hat{\mathbf{X}}^{(k)}\}$ representing a baseline probabilistic forecast. (B) Stage 2: Real-time contextual information $\mathbf{s}$, including structured and unstructured event data, is processed by an LLM. The LLM reasons about the event's impact and evaluates the candidate trajectories, selecting or adjusting to the most plausible event-conditioned forecast $\hat{\mathbf{X}}^*$. (C) Stage 3: The adjusted forecast, along with historical dispatch patterns, feeds into the LLM to generate human-readable summary reports and actionable traffic management recommendations.}
    \label{fig:1}
\end{figure}

\begin{itemize}
    \item \textbf{Stage 1: Foundational Probabilistic Forecasting (Fig.\ref{fig:1} A)}: The foundation of Chat-ITS is a robust forecasting model capable of capturing complex dependencies in traffic data and providing probabilistic outputs. We employ a state-of-the-art architecture pre-trained on extensive historical traffic data. To ensure the model learns representative patterns, the pre-training data include curated subsets, such as: (i) time-series from high-volume road segments or grid cells representing typical urban traffic dynamics, and (ii) traffic data aggregated around key venues (stadiums, transport hubs, event centers) known to generate non-standard patterns. Inspired by architectures like Chronos \cite{ansari_chronos_2024} which adapt language transformer-based models for time-series, our foundation model processes the historical input and generates not a single prediction, but multiple trajectory samples $\{\hat{\mathbf{X}}^{(k)}_{T+1:T+n}\}_{k=1}^K$. These samples collectively approximate the predictive distribution $P(\mathbf{X}_{T+1:T+n} | \mathbf{X}_{1:T})$, providing a baseline probabilistic forecast and inherent uncertainty quantification, crucial for representing the range of possibilities under routine conditions.
    
    \item \textbf{Stage 2: LLM-Enhanced Contextual Adjustment (Fig.\ref{fig:1} B)}: This stage integrates real-time contextual information $\mathbf{s}$ to refine the baseline forecast, addressing the limitations of models relying solely on historical patterns. The contextual information $\mathbf{s}$, which can include structured data and unstructured text, is processed by an LLM. The LLM executes a chain-of-thought process: first summarizing the event information, then reasoning about its likely causal impact on traffic flow (location, severity, duration), and finally assessing the quantitative effect. Then the LLM selects the most plausible trajectory $\hat{\mathbf{X}}^*_{T+1:T+n}$ or potentially generates an adjusted trajectory that better reflects the anticipated impact of the event. This step leverages the LLM's ability to understand and reason about novel or complex situations described in natural language, effectively modulating the initial probabilistic forecast based on real-time context.
    
    \item \textbf{Stage 3: LLM-Powered Reporting and Decision Support (Fig.\ref{fig:1} C)}: The final stage focuses on translating the adjusted forecast $\hat{\mathbf{X}}^*_{T+1:T+n}$ into practical outputs for end-users. The LLM receives the context-adjusted forecast and potentially relevant historical traffic guidance data. This historical guidance data allows the LLM to learn implicit operational preferences and common responses implemented by human traffic controllers in similar past situations. Based on the adjusted forecast, the contextual information, and the learned operational patterns, the LLM generates: (i) a concise, human-readable summary report describing the anticipated traffic conditions, highlighting potential issues (e.g., specific bottlenecks, expected delay increases), and explaining the reasoning based on the contextual factors; and (ii) actionable recommendations for traffic management (e.g., "Consider adjusting signal timing plan B on Corridor X between 8-10 AM," "Disseminate advisory regarding lane closure on Highway Y," "Prepare diversion route Z"). This stage bridges the gap between raw numerical prediction and practical operational utility, providing explainable insights and decision support. 
\end{itemize}


\subsection{Baseline Performance under Routine Conditions}
To establish the foundational capability of our framework, we first evaluated the performance of the pre-trained spatio-temporal model (Stage 1 output, prior to LLM adjustment) under routine traffic conditions, comparing it against established state-of-the-art deep learning baselines. The evaluation was conducted using datasets from Beijing covering both weekdays and weekend in January to May 2024, excluding periods identified with major anomalies. Baseline models included  DLinear \cite{zeng_are_2022}, FiLM \cite{zhou2022film}, Informer \cite{zhou2021informer}, PatchTST \cite{nie_time_2023}, Chronos \cite{ansari_chronos_2024}, and iTransformer \cite{liu_itransformer_2023}, trained on the same historical data. Performance was measured using standard forecasting metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), and Weighted Absolute Percentage Error (WAPE) for prediction horizons of 15, 30, and 60 minutes.

Our results, summarized in Table \ref{table:1}, demonstrate that the Chat-ITS foundational model substantial outperforms all evaluated deep learning baselines under these normal conditions. Across all prediction horizons (15, 30, and 60 minutes) and all metrics (MAE, MSE, WAPE), our model consistently achieved the lowest error rates, indicating superior accuracy. Notably, while Informer emerged as the most competitive baseline, particularly at shorter horizons, our model still surpassed its performance considerably. For instance, the average MAE for our model (0.153) was markedly lower than Informer's (0.166) and substantially better than other models like PatchTST (0.259) or FiLM (0.527).

Furthermore, our model displayed remarkable stability in performance across the different prediction horizons, maintaining consistently low error values even for 60-minute forecasts (e.g., MAE 0.158, MSE 0.058). This contrasts with several baseline models, such as DLinear and PatchTST, which exhibited a more pronounced degradation in accuracy as the forecast horizon increased. This superior baseline performance validates that the pre-trained foundation model effectively captures complex spatio-temporal dependencies from historical data. It provides a robust and highly accurate starting point for subsequent contextual adjustment via the LLM, achieving state-of-the-art predictive power even under typical operating conditions. The probabilistic nature of the output also provides valuable uncertainty estimates, a feature often lacking in deterministic baselines.

\input{tables/table1}

\subsection{Enhanced Prediction Accuracy across Diverse Anomalous Events}
A critical limitation of traditional traffic forecasting models is their reduced reliability during non-routine conditions. Chat-ITS is designed to address this by integrating contextual information via a Large Language Model (LLM) to adjust forecasts during anomalous events. We assessed this capability using two distinct real-world datasets, each representing different types of traffic disruptions with unique characteristics.The first dataset comprises scheduled highway construction projects. This data, sourced from Amap logs of transportation authority records, includes details known in advance, such as planned start/end times, location, and the number of lanes affected. These events are typically pre-planned and can have extended durations (days to weeks), often causing persistent, albeit partial, capacity reductions on specific road segments.The second dataset consists of unplanned traffic incidents reported by traffic police. This dataset contains unstructured natural language descriptions of events like accidents, breakdowns, or debris on the road. Unlike construction, these incidents are unforeseen, reported with some inherent delay after occurrence, and while potentially shorter in duration (hours), they can trigger abrupt and severe, localized disruptions, sometimes leading to full closures.

For both datasets, relevant contextual information $\mathbf{s}$ was formulated into natural language prompts for the LLM component of Chat-ITS. We then evaluated the final context-adjusted Chat-ITS forecasts against two key baselines: a Zero-shot prediction (the initial forecast from our foundation model before LLM adjustment) and a Few-shot approach (where the foundation model was fine-tuned on a subset of data containing anomalous events). The evaluation focused specifically on performance during these distinct types of disruptions. 

As shwon in Table \ref{table:2}, Chat-ITS demonstrated superior prediction accuracy compared to both baseline approaches across all forecast horizons. Compared to the Zero-shot forecast, Chat-ITS consistently reduced prediction errors. For example, the average MAE for Chat-ITS (5.27 km/h) was notably lower than that for the fine-tuned model (5.67 km/h). Crucially, Chat-ITS also outperformed the Few-shot/Finetuning strategy. While fine-tuning offers a conventional method for adapting models to specific conditions using historical data, it was less effective than the LLM-based contextual adjustment provided by Chat-ITS for these anomalous events. This finding suggests that the LLM's ability to interpret and reason from explicit, often real-time, contextual descriptions (like construction schedules or incident reports) provides a more potent adaptation mechanism than relying solely on learning from limited historical patterns of disruptions encountered during fine-tuning. While the average improvements in MAE/RMSE shown in the table are modest, the consistent outperformance across horizons and metrics, especially compared to fine-tuning, underscores the value of the approach. Furthermore, the impact can be more pronounced during specific high-impact events. These results highlight the effectiveness of the Chat-ITS framework. By leveraging an LLM to understand the nuances of diverse disruptions whether planned, long-term construction or sudden, severe incidents, and guide forecast adjustments accordingly, Chat-ITS delivers more reliable and accurate traffic predictions precisely when conventional models, even adapted ones, tend to fail.

Figure \ref{fig:2} provides illustrative examples from both the construction and incident datasets. These time-series plots visualize the actual traffic state (e.g., speed or volume) on affected road segments, alongside the uncertainty bounds predicted by the foundation model without contextual adjustment and the refined prediction generated by Chat-ITS with LLM-based correction. While the foundation model captures a plausible range of outcomes, its bounds are often too loose or misaligned with the real disruption patterns. In contrast, the context-adjusted predictions from Chat-ITS closely track the observed traffic dynamics during the anomaly periods, demonstrating the LLM's ability to enhance temporal precision and event awareness in forecasting.

\textbf{Case studies from the construction dataset:} In one scenario (Figure \ref{fig:2}a), the prompt described a "four-lane highway segment undergoing construction, with one lane closed for 4 hours (8 time steps) starting at 9:00 AM." The foundation model, finetuned on historical traffic patterns, underestimated the severity of the disruption. It predicted a moderate reduction in traffic volume and a gradual decline in speed, assuming partial capacity loss but no clear queue formation. However, Chat-ITS processed this text using the LLM, which inferred the critical impact of lane reduction on traffic flow dynamics. The LLM reasoned that the remaining three lanes would experience increased congestion due to reduced throughput, leading to localized gridlock during peak hours. Consequently, the LLM-adjusted forecast reflected a sharp drop in speed (from 60 km/h to below 20 km/h) and a sustained low-volume state for the duration of the closure, aligning with the observed traffic collapse. This adjustment captured the nonlinear effects of capacity reduction, which the foundation model failed to predict without explicit contextual input.

\textbf{Case studies from the incident dataset:} In another scenario (Figure \ref{fig:2}b), the prompt described a "major accident on a national expressway at 10:30 AM, causing full closure of the mainline and triggering traffic police recommendations to divert to a provincial bypass road (6 time steps)." The foundation model, relying on historical incident data, overestimated recovery times and assumed minimal impact on adjacent routes. Its prediction showed a temporary dip in speed followed by a rapid return to baseline levels. In contrast, the LLM processed the textual description of the accident and the diversion strategy, reasoning that the sudden closure would create a surge in traffic on the bypass road. The LLM adjusted the forecast to reflect a severe speed drop (to near-zero on the mainline) and a parallel increase in congestion on the bypass, which was validated by ground truth data. This demonstrated the LLM's ability to model cascading effects of incidents and incorporate real-time operational decisions (e.g., diversion routes) into the forecast.

These examples highlight how Chat-ITS leverages the LLM's contextual understanding to refine forecasts. For construction events, the LLM accounts for lane-specific capacity changes and peak-hour compounding effects. For incident scenarios, it integrates dynamic traffic management actions (e.g., diversions) to predict secondary congestion on alternative routes. By embedding domain-specific reasoning into the forecasting pipeline, Chat-ITS achieves higher accuracy in both short-term (6–8 time steps) and medium-term (up to 12 time steps) predictions compared to the foundation model alone. The improvements are particularly pronounced during high-impact events, where the LLM's explicit contextual interpretation compensates for the foundation model's reliance on historical patterns. This capability bridges the gap between statistical forecasting and operational reality, enabling Chat-ITS to deliver actionable insights during both planned disruptions and sudden anomalies.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figure2_anomaly_forecasts}
\caption{\textbf{Chat-ITS enhances forecasting accuracy during anomalous events through LLM-based contextual refinement.} Time-series comparison of predicted traffic states versus ground truth on affected road segments. \textbf{(a)}, Highway construction scenario with volume prediction. \textbf{(b)}, Traffic incident scenario with speed prediction. Shaded regions indicate the event durations. The black solid line represents ground truth observations. The red and blue dotted lines indicate the upper and lower prediction bounds from the foundation model without contextual information. The green dashed line shows the Chat-ITS prediction after LLM-based correction, which more accurately follows the observed disruption pattern.}
\label{fig:2}
\end{figure}

\input{tables/table2}

\subsection{Explainable Reporting and Actionable Recommendations}

Beyond raw predictive accuracy, a primary objective of Chat-ITS is to operationalize its forecasts by generating outputs that are directly explainable and useful for traffic management personnel. The standard output of a forecasting model—a matrix of future traffic states—lacks the essential context and prescriptive guidance needed for effective decision-making. Stage 3 of the framework directly addresses this "last-mile" problem by utilizing the LLM to translate the context-adjusted forecast from Stage 2 into human-readable reports and actionable operational recommendations. This stage leverages the LLM's sophisticated generative and reasoning capabilities, which can be optionally informed by few-shot examples of historical operational responses to align its suggestions with established best practices.

Figure \ref{fig:reports_recommendations} presents illustrative examples of these generated outputs, showcasing the system's ability to tailor its communication to the specific nature of the forecast.

\begin{itemize}
  \item \textbf{For a routine scenario (Figure \ref{fig:reports_recommendations}a)},  such as predictable morning peak congestion, the LLM provides a concise confirmation of the expected conditions. The report ("Expect typical heavy congestion...") serves as a valuable baseline, assuring operators that the system correctly identifies normal patterns. The accompanying recommendation ("Ensure ramp metering plan active.") is not merely a passive observation but a prompt to verify a standard operating procedure, demonstrating an understanding of routine traffic management protocols. This capability builds trust and establishes the system's reliability under normal circumstances.
  \item \textbf{In contrast, when presented with a forecast adjusted for a construction event (Figure \ref{fig:reports_recommendations}b)}, the LLM's output becomes more detailed and analytical. It synthesizes the quantitative forecast (the predicted drop in speed and volume) with the qualitative context ("single lane closure"). The resulting report goes beyond stating the problem; it explains the causal link ("...starting 8:00 AM due to lane closure"), quantifies the anticipated impact ("Delays likely exceeding 30 minutes..."), and even infers secondary consequences ("Adjacent alternate routes C and D expected to see increased volume."). The recommendations are correspondingly multi-faceted and proactive. They include public information dissemination ("Suggest deploying VMS advisory..."), strategic network control ("Consider implementing diversion strategy via Route C."), and tactical adjustments to mitigate ripple effects ("Adjust signal timings on Route C..."). This demonstrates a sophisticated level of reasoning that connects a localized event to its broader network-wide impact and proposes a coordinated, multi-pronged response.
  \item \textbf{Similarly, for a forecast adjusted for a major public event (Figure \ref{fig:reports_recommendations}c)}, the report highlights the specific timing and location of the anticipated egress congestion. The recommendations are highly tailored to this event type. Suggesting to "Implement special event signal timing plan B" implies an awareness of pre-defined operational playbooks, a critical feature for efficient management. Furthermore, the recommendation to "Monitor parking lot clearance rates" is a nuanced, event-specific metric that would not be relevant in a typical congestion scenario. This shows the LLM's ability to draw upon its understanding of the event's unique characteristics, potentially guided by historical patterns, to suggest highly relevant and targeted actions.
\end{itemize}

These case studies illustrate Chat-ITS's transformative capacity to bridge the critical gap between prediction and action. The system does not merely forecast traffic states; it synthesizes the numerical forecast and its underlying context (as interpreted by the LLM) into a coherent narrative. This translation from quantitative data to qualitative assessment and, ultimately, to prescriptive intelligence provides operators with clear, actionable guidance. This ability to generate context-aware, explainable reports and relevant recommendations represents a noticeable step towards more proactive, intelligent, and effective traffic management.

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{fig3.pdf}
  \caption{\textbf{LLM-generated explainable reports and actionable recommendations.} Examples showcasing Stage 3 outputs for different scenarios (e.g., a: Routine Congestion, b: Construction Event, c: Public Event Egress). Each panel displays the input context (brief description/reference), the resulting textual summary report assessing the situation, and the tailored actionable recommendations generated by the LLM, demonstrating the translation from context-aware prediction to operational intelligence.} \label{fig:reports_recommendations}
\end{figure}

\subsection{Ablation Studies}
To further  evaluate the contributions of individual components within the Chat-ITS framework, we performed ablation studies on the anomaly dataset. We systematically removed or altered key elements and measured the impact on forecasting accuracy during anomalous events.

First, we confirmed the necessity of the LLM-driven contextual adjustment (Stage 2).  Removing this stage and relying solely on the probabilistic forecast from the foundation model (Stage 1, e.g., using the median prediction) resulted in a noticeable degradation of accuracy during anomalous events (Figure \ref{fig:ablation_studies}a). Performance reverted to levels comparable to standard deep learning baselines, confirming that the LLM's ability to process contextual information is essential for adapting forecasts effectively when routine patterns are disrupted.

Second, we investigated the benefit of leveraging the probabilistic nature of the foundation model's forecast (Stage 1). Our foundation model generates a distribution of potential future trajectories. In the full Chat-ITS framework, the LLM uses its interpretation of the event's context (informed by characteristics like expected severity and duration) to select the most plausible trajectory from this distribution. We compared this to a variant where the LLM was only given a single, deterministic forecast (e.g., the mean prediction) to adjust. Results (Figure \ref{fig:ablation_studies}b) showed that providing the LLM with multiple candidate trajectories from the probabilistic forecast yielded better performance. This suggests that the probabilistic output provides a richer set of possibilities, allowing the LLM to make a more informed selection that better aligns with the contextually inferred impact, rather than attempting to drastically modify a single, potentially less suitable, baseline prediction.

Third, we examined the framework's sensitivity to the quality of the input context. Prompts describing anomalies were intentionally degraded (made vague or partially incorrect). While performance suffered compared to using accurate, detailed context, Chat-ITS still generally outperformed context-unaware baselines (Figure \ref{fig:ablation_studies}c). This indicates a degree of robustness but underscores the importance of high-quality, real-time contextual information for optimal performance.

Collectively, these ablation studies validate the synergistic design of Chat-ITS. They highlight the indispensable role of the LLM in interpreting external context for anomaly adaptation and demonstrate the added value of integrating this reasoning process with a robust, probabilistic foundation model that provides a range of plausible future scenarios.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{ablation_studies_refined_horizontal_xticks.pdf}
  \caption{\textbf{Ablation study results validating Chat-ITS components.} Performance comparison (e.g., MAE or RMSE during anomalous events on the anomaly dataset) evaluating: (a) The impact of removing the LLM contextual adjustment (Full Chat-ITS vs. Foundation Model only). (b) The benefit of using probabilistic input candidates versus a single deterministic input for LLM adjustment. (c) Sensitivity of Chat-ITS performance to varying context quality (accurate vs. vague/degraded). Results confirm the critical role of LLM reasoning and the value of the probabilistic foundation.} \label{fig:ablation_studies}
\end{figure}

\section{Discussion}
In this work, we introduced Chat-ITS, a hybrid framework that synergistically combines a pre-trained spatio-temporal foundation model for probabilistic forecasting with the contextual reasoning capabilities of Large Language Models to address key limitations in current traffic prediction systems. Our results demonstrate that Chat-ITS achieves forecasting accuracy comparable to state-of-the-art deep learning methods under routine traffic conditions. More importantly, it marked outperforms these baselines during anomalous events, such as construction, incidents, and public gatherings, by effectively incorporating real-time structured and unstructured contextual information via LLM processing. We showed substantial error reductions (up to 15\%) during such events, highlighting the framework's enhanced adaptability and resilience. Furthermore, case studies illustrated Chat-ITS's promising zero-shot generalization capability, allowing it to interpret and respond to novel event types described solely through natural language prompts. Finally, we demonstrated the framework's ability to generate explainable, human-readable summary reports and actionable traffic management recommendations, bridging the critical gap between prediction and operational decision-making.

The effectiveness of Chat-ITS stems from its deliberate hybrid design, which leverages the complementary strengths of deep learning for pattern recognition in high-dimensional spatio-temporal data and LLMs for flexible context understanding, reasoning, and natural language generation. The foundation model provides a statistically robust baseline forecast capturing complex recurring dynamics and uncertainty. The LLM, instead of being burdened with direct numerical prediction, focuses on its core strengths: interpreting diverse inputs (text, structured data), inferring causal impacts of events, evaluating scenarios based on context, and communicating findings effectively. This division of labor allows Chat-ITS to overcome the brittleness of purely data-driven models during anomalies and the limitations of purely LLM-based approaches in precise numerical forecasting. The integration of historical dispatch patterns further enhances the practical relevance of the generated recommendations. This synergistic approach represents a noticeable step towards ITS that are not only predictive but also adaptive, explainable, and operationally relevant.

The uniqueness of Chat-ITS lies in its synergistic integration of two powerful AI paradigms: deep learning for robust forecasting and LLMs for contextual reasoning. While deep learning models have pushed the boundaries of accuracy on benchmark datasets, they often lack robustness to out-of-distribution events and fail to provide actionable insights. Attempts to incorporate auxiliary data often rely on rigid input structures and struggle with unstructured text. LLM applications in transportation have primarily focused on tasks like route planning, dialogue systems, or summarizing traffic reports, but their direct application to end-to-end numerical forecasting remains challenging. Chat-ITS uniquely integrates these two powerful AI paradigms in a way that mitigates their respective weaknesses while harnessing their combined potential for context-aware, explainable, and actionable traffic prediction.

The performance of Chat-ITS during anomalies is inherently dependent on the availability and quality of real-time contextual information ($\mathbf{s}$). Inaccurate or delayed event reports will naturally limit the effectiveness of the LLM adjustment stage. While we demonstrated some robustness, further research is needed on handling noisy or conflicting contextual inputs. The reliance on LLMs also introduces computational costs associated with inference, although using smaller or optimized LLMs could mitigate this, and exploring different LLM architectures, including potentially smaller, domain-adapted models, could optimize this trade-off. Potential issues related to LLM biases or hallucinations, while mitigated by grounding the LLM's task in evaluating pre-generated trajectories, require ongoing vigilance and potentially safety layers in operational deployment. Furthermore, effective prompt engineering is crucial for eliciting the desired reasoning and output from the LLM, which may require domain expertise. Scalability to extremely large, city-wide networks with tens of thousands of sensors also needs further investigation.

Future research will focus on integrating Chat-ITS with real-time traffic control systems (e.g., adaptive signal control, variable speed limits) to enable fully automated, context-aware traffic management. We also aim to incorporate a wider range of contextual data sources, such as real-time social media feeds, advanced weather nowcasting, or connected vehicle data, to further enhance situational awareness. Developing more sophisticated methods for the LLM to not just select but actively modify forecast trajectories based on context could yield further accuracy gains. Finally, conducting user studies with traffic operators to evaluate the usability and effectiveness of the generated reports and recommendations in real-world control room settings is essential for practical validation and refinement.

\section{Materials and Methods}
\subsection{Datasets and Preprocessing}

This study utilizes multiple large-scale, real-world traffic and contextual datasets collected by Amap across China, primarily focusing on Beijing for foundational model training and broader regions for event context and specific analyses. All datasets cover the period from September 2023 to May 2024, unless otherwise specified.

\textbf{Spatio-Temporal Traffic Data:} The core dataset for training the probabilistic foundation model consists of high-resolution traffic state information for the urban core of Beijing. Raw traffic data was aggregated onto a regular grid with a spatial resolution of $500m \times 500m$. This resulted in $N=5,797$ distinct spatial grid cells covering the main urban road network. For each cell, key traffic state variables, including traffic volume (vehicles per interval) and average speed (km/h), were computed and aggregated into 5-minute intervals. This dataset comprises approximately 1.3 billion data points, providing a comprehensive representation of urban traffic dynamics much larger than many commonly used open-source benchmarks \cite{liu_largest_2023}.

\textbf{Venue-Centric Traffic Data:} To specifically capture traffic patterns influenced by large-scale public events, supplementary datasets focused on major venues were compiled.
\begin{itemize}
    \item \textit{Aggregated Venue Flow:} Derived from location-based service data, aggregated traffic volume information was obtained for the precise geographical boundaries and surrounding buffer areas of over 300 major venues (sports stadiums, concert halls, major tourist attractions, transport hubs) across multiple cities in China. This dataset helps model event-specific demand surges and dispersion patterns.
    \item \textit{Fine-Grained Venue Grid Data:} For a subset of the venues above, traffic volume was aggregated onto a finer $100m \times 100m$ grid covering the venue. Due to the high granularity leading to sparsity, we identified and utilized data primarily from the top-20 grid cells exhibiting the highest average historical traffic volume within each venue's defined area, focusing analysis on the most relevant micro-locations.
\end{itemize}

\textbf{Contextual Event Data:} Real-time and historical event information, crucial for the LLM reasoning stage (Stage 2) and evaluation during anomalies, was primarily sourced from the Amap open platform APIs and associated historical logs for the relevant periods and geographical areas. This included:
\begin{itemize}
    \item \textit{Structured Construction Data:} A curated dataset encompassing over 10,000 construction events on highways and major arterials. Each entry typically includes precise location information (coordinates or road segment identifiers), scheduled start and end times, number and type of lanes affected (e.g., closure, partial blockage), and nature of the work.
    \item \textit{Unstructured Anomaly Reports:} Traffic incident information disseminated via Amap, originating from user reports or official traffic authority alerts. These reports typically contain a natural language description of the incident (e.g., "Accident involving two cars on Ring Road eastbound near Exit 5, blocking right lane"), an approximate location , and a timestamp. This text serves as direct input for the LLM.
\end{itemize}

\textbf{Preprocessing:} Standard preprocessing steps were applied to the traffic datasets before model training and evaluation. Missing values in the time-series data less than 5\% of points were imputed using linear interpolation while others are dropped. Traffic state features (speed, volume) were normalized using Z-score normalization based on the mean and standard deviation calculated from the training portion of the primary Beijing dataset. For GNN-based baselines, the spatial graph adjacency matrix was constructed based on road network distance threshold, with edge weights typically defined by inverse distance. Unstructured text data from anomaly reports and event information was cleaned to remove irrelevant artifacts before being fed into the LLM prompts.

\subsection{Spatio-Temporal Foundation Model for Probabilistic Forecasting}
The foundation of our Chat-ITS framework is a powerful probabilistic forecaster built upon the principles of the Chronos framework \cite{ansari_chronos_2024}. This approach reframes forecasting as a {language modeling task. The core idea is to translate a continuous numerical time series into a sequence of discrete tokens, analogous to words in a sentence, and then use a standard language model architecture to learn the grammar of traffic dynamics and predict future tokens. For this purpose, we utilized a T5-based encoder-decoder architecture \cite{raffel2020exploring}, which we pre-trained on extensive historical traffic data.

The process involves three key stages: input tokenization, model training, and probabilistic forecast generation.

\begin{enumerate}
    \item \textbf{Time-Series Tokenization:} To make numerical data processable by a language model, each time series undergoes a two-step tokenization pipeline:
    \begin{itemize}
        \item \textbf{Scaling:} To handle the varying scales of traffic data, each time series is normalized. We employ mean scaling, where each value in the series ($x_t$) is divided by the mean of the absolute values of its historical context ($s$). This brings diverse series to a comparable scale.
        \begin{equation}
            x'_t = \frac{x_t}{s}
            \label{eq:scaling}
        \end{equation}
        
        \item \textbf{Quantization:} The scaled, continuous values ($x'_t$) are then mapped into a finite vocabulary of $B$ discrete integer tokens. This is achieved by dividing the range of possible scaled values into $B$ predefined bins. This critical step completes the transformation from a sequence of numbers to a sequence of tokens.
    \end{itemize}

    \item \textbf{Model Architecture and Training:} We employ a standard T5 encoder-decoder architecture, which is based on the Transformer model. The encoder processes the sequence of tokens representing historical traffic data to create a rich contextual representation. The core of the Transformer is the \textbf{Scaled Dot-Product Attention} mechanism, which allows the model to weigh the importance of different tokens in the input sequence when making a prediction. The attention output is computed as:
    \begin{equation}
        \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
        \label{eq:attention}
    \end{equation}
    where $\mathbf{Q}$ (Query), $\mathbf{K}$ (Key), and $\mathbf{V}$ (Value) are matrices derived from the input token embeddings, and $d_k$ is the dimension of the keys. The model further enhances this with Multi-Head Attention, running the attention mechanism in parallel multiple times to jointly attend to information from different representation subspaces.
    
    The decoder then uses the encoder's output to autoregressively generate the forecast token by token. The model is trained to predict the next token by minimizing the negative log-likelihood (cross-entropy loss) over the vocabulary of $B$ bins. The objective is formalized as:
    \begin{equation}
        L(\theta) = - \sum_{t=T+1}^{T+n} \log P(z_t | z_{<t}; \theta)
        \label{eq:loss}
    \end{equation}
    where $z_t$ is the ground-truth token at a future time step $t$, $z_{<t}$ represents all preceding tokens, and $P(\cdot)$ is the probability distribution predicted by the model with parameters $\theta$. This process effectively trains the model to perform "regression via classification."

    \item \textbf{Probabilistic Forecast Generation:} At inference time, the trained model generates probabilistic forecasts. For each future time step, the decoder outputs a probability distribution across all $B$ tokens. To capture uncertainty, we generate multiple future scenarios by autoregressively \textbf{sampling} from this distribution $K$ times (in our work, $K=20$). These sampled token sequences are then converted back into numerical trajectories through a reverse pipeline:
    \begin{itemize}
        \item \textbf{De-quantization:} Each token is mapped back to the numerical center of its corresponding bin.
        \item \textbf{Re-scaling:} The values are multiplied by the original scaling factor ($s$) to restore them to their physical scale.
    \end{itemize}
    This procedure yields a set of $K$ distinct future trajectories, which collectively form a probabilistic forecast. This serves as the crucial input for the LLM-based contextual adjustment in Stage 2 of our framework.
\end{enumerate}

\subsection{LLM Integration Details}
We primarily utilized \textit{Qwen2.5-14B-Instruct} \cite{qwen} for the LLM components in Stages 2 and 3, selected for its strong reasoning and instruction-following capabilities. Structured data (weather, road work) was formatted as key-value pairs. Unstructured text (incidents, events) was included directly, prefixed with source and time. Time-series candidate trajectories were summarized by providing key statistics (e.g., min/max/avg speed in critical zones, predicted congestion duration).In the primary mode, the LLM selected the single most plausible trajectory ($k^*$) from the K candidates based on its contextual evaluation: $\hat{\mathbf{X}}^*_{T+1:T+n} = \hat{\mathbf{X}}^{(k^*)}_{T+1:T+n}$. In experiments exploring active adjustment, the LLM's quantitative impact assessment (e.g., predicted \% speed reduction) was used to mathematically modify the selected or mean trajectory. Reporting/Recommendation Generation: Standard LLM text generation was used based on the prompts described above. Few-shot examples from the historical dispatch data were included in the recommendation prompts to bias the LLM towards operationally relevant suggestions. No LLM fine-tuning was performed for this study. Besides, API calls were made using standard libraries with default temperature settings (e.g., temperature=0.7) for generative tasks in Stage 3 to allow for some variability, and lower temperature (e.g., 0.1) for the selection task in Stage 2 to ensure consistent choices.

\subsection{Baseline Implementations}
The chosen baselines represent a broad spectrum of modern time series forecasting methodologies, ensuring a robust and comprehensive comparison against different architectures:

\begin{itemize}
    \item DLinear\cite{zeng_are_2022}: Represents simple yet surprisingly effective linear models, serving as a strong benchmark against more complex architectures by decomposing the time series and applying separate linear layers. It challenges the necessity of intricate designs for certain forecasting tasks.
    \item FiLM \cite{zhou2022film}: Represents linear models enhanced with frequency analysis, designed to improve forecasting by better capturing periodicity through specific decomposition techniques applied in the frequency domain.
    \item Informer \cite{zhou2021informer}: A prominent Transformer-based model optimized for long sequence time-series forecasting (LSTF) efficiency through a ProbSparse self-attention mechanism and distilling operation, representing efficient Transformer variants.
    \item PatchTST \cite{nie_time_2023}: Represents channel-independent Transformer approaches utilizing patching, where input time series are divided into subseries-level patches that are fed as tokens to the Transformer, capturing local semantic information.
    \item Chronos \cite{ansari_chronos_2024}: Represents recent large pre-trained foundation models for time series, leveraging language model architectures scaled to time series data for zero-shot or few-shot forecasting, showcasing the potential of large-scale pre-training.
    \item iTransformer \cite{liu_itransformer_2023}: An innovative Transformer architecture that inverts the standard process by applying attention to embedded variates across the entire time series length, designed to better capture multivariate correlations.
\end{itemize}

For implementation, we utilized established frameworks such as TSLib \cite{wu2023timesnet, wang2024tssurvey} or the official public code repositories associated with each baseline model. To guarantee a fair and direct comparison, all baseline models were trained using the identical historical dataset that was employed for training the Chat-ITS foundation model. The sole exception was Chronos, for which we leveraged the publicly available pre-trained weights, applying it directly as a zero-shot forecaster without fine-tuning on our specific dataset. This curated selection of baselines ensures our evaluation covers a diverse range of contemporary forecasting architectures, encompassing simple linear approaches, frequency-domain enhanced models, various Transformer adaptations (including those optimized for efficiency, employing patching mechanisms, or utilizing inverted attention across variates), and large pre-trained foundation models.

\subsection{Evaluation Details}
We evaluated the model's performance using standard time-series forecasting metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), and Weighted Absolute Percentage Error (WAPE). MAE measures the average absolute difference between predictions and actual values. MSE computes the average of the squared errors, which emphasizes larger deviations more heavily. RMSE is the square root of MSE, bringing the error back to the original scale of the data. MAPE and WAPE assess the relative size of errors compared to actual values, providing scale-independent evaluation.

It is worth noting that we preferred WAPE over MAPE for traffic volume forecasting tasks, where the data often contains zero or near-zero values. In such scenarios, MAPE can become undefined or unstable due to division by zero or extremely small actual values, leading to misleading evaluations. Additionally, in cases with large variance in traffic volumes, MAPE tends to overemphasize errors in low-volume periods while underrepresenting high-volume ones. In contrast, WAPE normalizes total absolute error by the sum of actual values across all time steps and locations, offering a more stable and representative metric under these conditions.

The specific metrics are defined as \eqref{mae}, \eqref{mse}, \eqref{rmse}, \eqref{mape}, and \eqref{wape}:

\begin{equation} \label{mae}
\text{MAE} = \frac{1}{nN} \sum_{t=T+1}^{T+n} \sum_{i=1}^{N} |x_{t,i} - \hat{x}{t,i}|
\end{equation}
\begin{equation} \label{mse}
\text{MSE} = \frac{1}{nN} \sum{t=T+1}^{T+n} \sum_{i=1}^{N} (x_{t,i} - \hat{x}{t,i})^2
\end{equation}
\begin{equation} \label{rmse}
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{ \frac{1}{nN} \sum{t=T+1}^{T+n} \sum_{i=1}^{N} (x_{t,i} - \hat{x}{t,i})^2 }
\end{equation}
\begin{equation} \label{mape}
\text{MAPE} = \frac{1}{nN} \sum{t=T+1}^{T+n} \sum_{i=1}^{N} \left| \frac{x_{t,i} - \hat{x}{t,i}}{x{t,i}} \right|
\end{equation}
\begin{equation} \label{wape}
\text{WAPE} = \frac{\sum_{t=T+1}^{T+n} \sum_{i=1}^{N} |x_{t,i} - \hat{x}{t,i}|}{\sum{t=T+1}^{T+n} \sum_{i=1}^{N} |x_{t,i}|}
\end{equation}

\textbf{Anomaly Identification}: For evaluating performance during anomalies, event periods were identified using timestamps from the Amap/Gaode construction and incident logs.  Anomalous periods were defined as 1 hour before to 1 hours after the logged event time for relevant locations. For public events, the anomalous period covered 2 hours before the event start to 2 hours after the event end. Zero-shot evaluation used events from categories completely held out during any training/fine-tuning. 

\textbf{Data Splits}: Data was split chronologically for each city/dataset. Typically, the first 70\% was used for pre-training the foundation model, the next 10\% for validation (e.g., selecting foundation model checkpoints, basic prompt tuning), and the final 20\% for testing. 

\section*{Acknowledgments}

This work was supported by Beijing Natural Science Foundation (No. JQ24051), Beijing Nova Program (No. 20230484432) and Independent Research Project of the State Key Laboratory of Intelligent Green Vehicle and Mobility, Tsinghua University (No. ZZ-GG-20250406).

\section{Data Availability}

The datasets central to this study, including the large-scale traffic network data and the anomalous event logs, were provided by Amap. Due to the proprietary nature of this information, which encompasses commercial sensitivities and privacy considerations, these datasets are not publicly available. We acknowledge the importance of reproducibility and regret that these necessary restrictions prevent the public dissemination of these materials. Enquiries regarding the methodology or potential collaborations may be directed to the corresponding author.

\section{Supplementary Materials}
\begin{itemize}
    \item \textbf{Text S1 and Table S1.} Detailed description and illustrative examples of the Structured Construction Data used in this study.

    \item \textbf{Text S2 and Table S2.} Detailed description and examples of the Unstructured Anomaly Reports.
\end{itemize}

\printbibliography

\end{document}